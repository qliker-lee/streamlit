# -------------------------------------------------
# 25_Value Chain & System Definition.py ì—ì„œ ì…ë ¥ëœ ë‚´ìš©ì„ ë¶„ì„
# 2025.12.26 Qliker 
# -------------------------------------------------
# -------------------------------------------------------------------
# 1. ê²½ë¡œ ì„¤ì • (Streamlit warnings import ì „ì— í•„ìš”)
# -------------------------------------------------------------------
import sys
from pathlib import Path

CURRENT_DIR = Path(__file__).resolve()
PROJECT_ROOT = CURRENT_DIR.parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))
# -------------------------------------------------------------------
# 2. Streamlit ê²½ê³  ì–µì œ ì„¤ì • (Streamlit import ì „ì— í˜¸ì¶œ)
# -------------------------------------------------------------------
from DataSense.util.streamlit_warnings import setup_streamlit_warnings
setup_streamlit_warnings()

from DataSense.util.Display import create_metric_card # KPI ë©”íŠ¸ë¦­ í‘œì‹œ í•¨ìˆ˜
# -------------------------------------------------------------------
# 3. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
# -------------------------------------------------------------------
import streamlit as st
import pandas as pd
import plotly.express as px
import os
from pathlib import Path


SOLUTION_NAME = "Value Chain & System Analysis"
SOLUTION_KOR_NAME = "Value Chain & System Analysis"
APP_NAME = "Value Chain & System Analysis"
APP_DESC = "###### Value Chain & Systemë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° íŒŒì¼ë“¤ì— ëŒ€í•œ í†µê³„ ì •ë³´ì…ë‹ˆë‹¤.  "

from DataSense.util.Files_FunctionV20 import load_yaml_datasense, set_page_config
set_page_config(APP_NAME)

# -------------------------------------------------
# 3. Streamlit í˜ì´ì§€ ì„¤ì •
# ------------------------------------------------- 
# ê²½ë¡œ ì„¤ì • (Pathlib í™œìš©)
PROJECT_ROOT = Path(__file__).resolve().parents[1]
BASE_PATH = PROJECT_ROOT / "DataSense"
OUTPUT_DIR = BASE_PATH / "DS_Output"
VC_FILE = OUTPUT_DIR / "DS_ValueChain.csv"
SYS_FILE = OUTPUT_DIR / "DS_System.csv"
VC_SYS_FILE = OUTPUT_DIR / "DS_ValueChain_System_File.csv"
MAPPING_FILE = OUTPUT_DIR / "CodeMapping.csv"

# -------------------------------------------------
# 4. ë°ì´í„° ë¡œë“œ
# ------------------------------------------------- 
# @st.cache_data
def load_data(file_path):
    """íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤. íŒŒì¼ì´ ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not os.path.exists(file_path):
        return None
    try:
        return pd.read_csv(file_path)
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {str(e)}")
        return None

def get_file_summary(file_names, df_mapping):
    """ì„ íƒëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ FileName, ColumnCnt, PK Listë¥¼ ì¶”ì¶œ"""
    if df_mapping is None or len(file_names) == 0:
        return pd.DataFrame(columns=['FileName', 'ColumnCnt', 'PK_List'])
    
    relevant_mapping = df_mapping[df_mapping['FileName'].isin(file_names)]
    summary = []
    for f_name in file_names:
        f_data = relevant_mapping[relevant_mapping['FileName'] == f_name]
        col_cnt = len(f_data)
        
        # PK ì»¬ëŸ¼ ì¶”ì¶œ (PK ê°’ì´ 1ì¸ ì»¬ëŸ¼ë“¤)
        pk_str = "-"
        if 'PK' in f_data.columns:
            pk_cols = f_data[f_data['PK'].astype(str).str.contains('1', na=False)]['ColumnName'].tolist()
            if pk_cols:
                pk_str = ", ".join(pk_cols)
            
        summary.append({
            'FileName': f_name,
            'ColumnCnt': col_cnt,
            'PK_List': pk_str
        })
    
    return pd.DataFrame(summary)

def load_data_validation():
    """ í•„ìš”í•œ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. """
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë¨¼ì € í™•ì¸
    missing_files = []
    if not os.path.exists(VC_FILE):
        missing_files.append(f"VC_FILE: {VC_FILE}")
    if not os.path.exists(SYS_FILE):
        missing_files.append(f"SYS_FILE: {SYS_FILE}")
    if not os.path.exists(VC_SYS_FILE):
        missing_files.append(f"VC_SYS_FILE: {VC_SYS_FILE}")
    if not os.path.exists(MAPPING_FILE):
        missing_files.append(f"MAPPING_FILE: {MAPPING_FILE}")
    
    if missing_files:
        st.error("ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
        for file_info in missing_files:
            st.error(f"  - {file_info}")
        return None, None
    
    # íŒŒì¼ ë¡œë“œ
    df_vc = load_data(VC_FILE)
    df_sys = load_data(SYS_FILE)
    df_vc_sys = load_data(VC_SYS_FILE)
    df_mapping = load_data(MAPPING_FILE)

    # ë¡œë“œ ê²°ê³¼ í™•ì¸
    failed_files = []
    if df_vc is None:
        failed_files.append(f"VC_FILE: {VC_FILE}")
    if df_sys is None:
        failed_files.append(f"SYS_FILE: {SYS_FILE}")
    if df_vc_sys is None:
        failed_files.append(f"VC_SYS_FILE: {VC_SYS_FILE}")
    if df_mapping is None:
        failed_files.append(f"MAPPING_FILE: {MAPPING_FILE}")
    
    if failed_files:
        st.error("ë‹¤ìŒ íŒŒì¼ë“¤ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
        for file_info in failed_files:
            st.error(f"  - {file_info}")
        return None, None, None, None

    df_vc_sys = pd.merge(df_vc_sys, df_vc, on=['Industry', 'Activity'], how='left')
    df_vc_sys = pd.merge(df_vc_sys, df_sys, on=['Industry', 'System'], how='left')
    df_vc_sys = df_vc_sys.dropna(subset=['Activity', 'System'])
    df_vc_sys = df_vc_sys[(df_vc_sys['Activity'] != 'Unknown') & (df_vc_sys['System'] != 'Unknown')]
    df_vc_sys = df_vc_sys.sort_values(['Activity_Seq', 'System_Seq'], ascending=True)

    return df_vc, df_sys, df_vc_sys, df_mapping

def select_industry(df_vc, df_sys, df_vc_sys):
    
    col_sel1, col_sel2 = st.columns([1, 1])
    with col_sel1:
        st.header("ğŸ¢ Industry Selection")

    with col_sel2:
        industries = sorted(df_vc_sys['Industry'].unique())
        selected_industry = st.selectbox("ë¶„ì„í•  ì‚°ì—…êµ°ì„ ì„ íƒí•˜ì„¸ìš”", industries)
        df_ind = df_vc_sys[df_vc_sys['Industry'] == selected_industry]
        df_sys = df_sys[df_sys['Industry'] == selected_industry]
        df_vc = df_vc[df_vc['Industry'] == selected_industry]

    if df_ind is not None:
        summary = {
            "Activity #": len(df_vc['Activity'].unique()),
            "System #": len(df_sys['System'].unique()),
            "File #": len(df_ind['FileName'].unique())
        }

        # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•œ ìƒ‰ìƒ ì •ì˜
        metric_colors = {
            "Activity #": "#1f77b4",
            "System #": "#2ca02c", 
            "File #": "#ff7f0e"
        }
        cols = st.columns(len(summary))
        for col, (key, value) in zip(cols, summary.items()):
            color = metric_colors.get(key, "#0072B2") # ê¸°ë³¸ ìƒ‰ìƒ
            col.markdown(create_metric_card(value, key, color), unsafe_allow_html=True)
        return selected_industry, df_ind

def activity_analysis(df_ind, df_mapping, all_activities):
    st.header(f"âš™ï¸ Activity Analysis")
    # Activity_Seq ìˆœìœ¼ë¡œ ì •ë ¬
    act_counts = df_ind.groupby('Activity')['FileName'].count().reset_index()
    # Activity_Seqë¥¼ ê°€ì ¸ì™€ì„œ mergeí•˜ì—¬ ì •ë ¬
    activity_seq = df_ind[['Activity', 'Activity_Seq']].drop_duplicates()
    act_counts = act_counts.merge(activity_seq, on='Activity', how='left')
    act_counts = act_counts.sort_values('Activity_Seq', ascending=True)

    act_col1, act_col2 = st.columns([3, 3])
    with act_col1:
        act_tab1, act_tab2, act_tab3 = st.tabs(["Activityë³„ íŒŒì¼ ë¶„í¬(íŒŒì´ ì°¨íŠ¸)", 
        "Activityë³„ íŒŒì¼ ìˆ˜(ë§‰ëŒ€ ì°¨íŠ¸)", "Activityë³„ íŒŒì¼ ìˆ˜(í…Œì´ë¸”)"])
        with act_tab1:  
            # # íŒŒì´ ì°¨íŠ¸ ìƒì„± (ë„ë„› í˜•íƒœ) 
            fig_act = px.pie(act_counts, names='Activity', values='FileName', 
                            title=f"Activityë³„ íŒŒì¼ ë¶„í¬",
                            hole=0.4, # ë„ë„› í˜•íƒœ
                            color_discrete_sequence=px.colors.qualitative.Pastel,
                            category_orders={'Activity': act_counts['Activity'].tolist()})
            fig_act.update_traces(textposition='inside', textinfo='percent+label', sort=False)
            st.plotly_chart(fig_act, width="stretch")

        with act_tab2:
            # ë§‰ëŒ€ ì°¨íŠ¸ ìƒì„± (Activityë³„ íŒŒì¼ ìˆ˜) 
            fig_act = px.bar(act_counts, x='Activity', y='FileName', 
                            title=f"Activityë³„ íŒŒì¼ ìˆ˜",
                            color='Activity', height=400)
            fig_act.update_layout(bargap=0.2, showlegend=False)
            st.plotly_chart(fig_act, width="stretch")
        with act_tab3:
            st.dataframe(act_counts, width="stretch", height=400, hide_index=True)

    with act_col2:
        selected_act = st.selectbox("Activityë¥¼ ì„ íƒí•˜ì„¸ìš”", all_activities, key="sel_act")
        # st.subheader(f"ğŸ“„ '{selected_act}' Activityì— ì†í•œ íŒŒì¼ ìš”ì•½")
        act_files = df_ind[df_ind['Activity'] == selected_act]['FileName'].unique()
        act_summary = get_file_summary(act_files, df_mapping)
        st.dataframe(act_summary, width="stretch", height=400, hide_index=True)

    st.divider()


def system_analysis(df_ind, df_mapping, all_systems):
    st.header(f"ğŸ’» System Analysis")
    
    # System_Seq ìˆœìœ¼ë¡œ ì •ë ¬
    sys_counts = df_ind.groupby('System')['FileName'].count().reset_index()
    # System_Seqë¥¼ ê°€ì ¸ì™€ì„œ mergeí•˜ì—¬ ì •ë ¬
    system_seq = df_ind[['System', 'System_Seq']].drop_duplicates()
    sys_counts = sys_counts.merge(system_seq, on='System', how='left')
    sys_counts = sys_counts.sort_values('System_Seq', ascending=True)
    sys_col1, sys_col2 = st.columns([3, 3])
    
    with sys_col1:
        sys_tab1, sys_tab2, sys_tab3 = st.tabs(["Systemë³„ íŒŒì¼ ë¶„í¬(íŒŒì´ ì°¨íŠ¸)", 
                    "Systemë³„ íŒŒì¼ ìˆ˜(ë§‰ëŒ€ ì°¨íŠ¸)", "Systemë³„ íŒŒì¼ ìˆ˜(í…Œì´ë¸”)"])
        with sys_tab1:
            # íŒŒì´ ì°¨íŠ¸ ìƒì„±
            fig_sys = px.pie(sys_counts, names='System', values='FileName', 
                            title=f"Systemë³„ íŒŒì¼ ë¶„í¬",
                            hole=0.4, # ë„ë„› í˜•íƒœ
                            color_discrete_sequence=px.colors.qualitative.Pastel)
            fig_sys.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig_sys, width="stretch")
        with sys_tab2:
            # ë§‰ëŒ€ ì°¨íŠ¸ ìƒì„± (Systemë³„ íŒŒì¼ ìˆ˜)
            fig_sys = px.bar(sys_counts, x='System', y='FileName', 
                            title=f"Systemë³„ íŒŒì¼ ìˆ˜",
                            color='System', height=400)
            st.plotly_chart(fig_sys, width="stretch")
        with sys_tab3:
            st.dataframe(sys_counts, width="stretch", height=400, hide_index=True)

    with sys_col2:
        selected_sys = st.selectbox("Systemì„ ì„ íƒí•˜ì„¸ìš”", all_systems, key="sel_sys")
        sys_files = df_ind[df_ind['System'] == selected_sys]['FileName'].unique()
        sys_summary = get_file_summary(sys_files, df_mapping)
        st.dataframe(sys_summary, width="stretch", height=400, hide_index=True)

#-----------------------------------------------------------------------------------------
def Display_MasterFormat_Detail(ff_df):
    """Master Format Detail í™”ë©´ ì¶œë ¥"""

    # ê° ë·°ë³„ ì»¬ëŸ¼ ì •ì˜
    VIEW_COLUMNS = {
        "Value Info": [
            'FileName', 'ColumnName', 'OracleType', 'PK', 'ValueCnt',
            'Null(%)', 'UniqueCnt', 'Unique(%)',
            'MinString', 'MaxString', 'ModeString', # 'MedianString', 'ModeCnt', 'Mode(%)'
        ],
        "Value Type Info": [
            'FileName', 'ColumnName', 'ValueCnt', 'FormatCnt',
            'Format', 'Format(%)', 'FormatMin', 'FormatMax', 'FormatMode', 'FormatMedian',
            'Format2nd', 'Format2nd(%)', 'Format2ndMin', 'Format2ndMax', 'Format2ndMode', 'Format2ndMedian',
            'Format3rd', 'Format3rd(%)'
        ],

        "Top10 Info": [
            'FileName', 'ColumnName', 'ValueCnt', 'ModeString', 'ModeCnt', 'Mode(%)',
            'Top10', 'Top10(%)'
        ],
        "Length Info": [
            'FileName', 'ColumnName', 'OracleType', 'PK', 'DetailDataType',
            'LenCnt', 'LenMin', 'LenMax', 'LenAvg', 'LenMode',
            'RecordCnt', 'SampleRows', 'ValueCnt', 'NullCnt', 'Null(%)',
            'UniqueCnt', 'Unique(%)'
        ],
        "Character Info": [
            'FileName', 'ColumnName', 'ValueCnt', 'HasBrokenKor', 'HasSpecial', 'HasUnicode', 'HasChinese', 
            'HasTab', 'HasCr', 'HasLf', 'HasJapanese', 'HasBlank', 'HasDash', 'HasDot', 'HasAt', 'HasAlpha',
            'HasKor', 'HasNum', 'HasBracket', 'HasMinus', 'HasOnlyAlpha', 'HasOnlyNum',
            'HasOnlyKor', 'HasOnlyAlphanum',
            'FirstChrKor', 'FirstChrNum', 'FirstChrAlpha', 'FirstChrSpecial'
        ],
        "DQ Score Info": [
            'FileName', 'ColumnName', 'ValueCnt', 'Null_pct', 'TypeMixed_pct', 'LengthVol_pct', 'Duplicate_pct',
            'DQ_Score', 'DQ_Issues', 'Issue_Count'
        ]
    }

    # ---------------------------
    st.markdown("### Data Quality Information")
    st.markdown("###### ì•„ë˜ì˜ íƒ­ì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    if ff_df.empty:
        st.warning("Data Quality ë¶„ì„ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    if ff_df is not None and not ff_df.empty:
        tabs = ['Value Info', 'Value Type Info', 'Top10 Info', 'Length Info', 
            'Character Info', 'DQ Score Info', 'Total Statistics']
        tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs(tabs)

        with tab1:
            st.markdown("###### ëª¨ë“  ì»¬ëŸ¼ë“¤ì˜ ë°ì´í„° ê°’ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
            available_cols = [col for col in VIEW_COLUMNS['Value Info'] if col in ff_df.columns]
            if available_cols:
                df = ff_df[available_cols].reset_index(drop=True)
                st.dataframe(data=df, width=1400, height=600, hide_index=True)
            else:
                st.warning("í‘œì‹œí•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        with tab2:
            st.markdown("###### ëª¨ë“  ì»¬ëŸ¼ë“¤ì˜ ë°ì´í„° íƒ€ì… ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
            available_cols = [col for col in VIEW_COLUMNS['Value Type Info'] if col in ff_df.columns]
            if available_cols:
                df = ff_df[available_cols].reset_index(drop=True)
                st.dataframe(data=df, width=1400, height=600, hide_index=True)
            else:
                st.warning("í‘œì‹œí•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        with tab3:
            st.markdown("###### ëª¨ë“  ì»¬ëŸ¼ë“¤ì˜ ë¹ˆë„ìˆ˜ ìƒìœ„ 10ê°œë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
            available_cols = [col for col in VIEW_COLUMNS['Top10 Info'] if col in ff_df.columns]
            if available_cols:
                df = ff_df[available_cols].reset_index(drop=True)
                st.dataframe(data=df, width=1400, height=600, hide_index=True)
            else:
                st.warning("í‘œì‹œí•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        with tab4:
            st.markdown("###### ëª¨ë“  ì»¬ëŸ¼ë“¤ì˜ ê¸¸ì´ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
            available_cols = [col for col in VIEW_COLUMNS['Length Info'] if col in ff_df.columns]
            if available_cols:
                df = ff_df[available_cols].reset_index(drop=True)
                st.dataframe(data=df, width=1400, height=600, hide_index=True)
            else:
                st.warning("í‘œì‹œí•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        with tab5:
            st.markdown("###### ëª¨ë“  ì»¬ëŸ¼ë“¤ì˜ êµ¬ì„±í•˜ëŠ” ë¬¸ì ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
            available_cols = [col for col in VIEW_COLUMNS['Character Info'] if col in ff_df.columns]
            if available_cols:
                df = ff_df[available_cols].reset_index(drop=True)
                st.dataframe(data=df, width=1400, height=600, hide_index=True)
            else:
                st.warning("í‘œì‹œí•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        with tab6:
            st.markdown("###### ëª¨ë“  ì»¬ëŸ¼ë“¤ì˜ Data Quality Score ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. (ê¸°ì—…ì˜ ìƒí™©ì— ë”°ë¼ ê¸°ì¤€ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì»¨ì„¤íŒ… í›„ í™•ì •í•©ë‹ˆë‹¤.)")
            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
            available_cols = [col for col in VIEW_COLUMNS['DQ Score Info'] if col in ff_df.columns]
            if available_cols:
                df = ff_df[available_cols].reset_index(drop=True)
                st.dataframe(data=df, width=1400, height=600, hide_index=True)
            else:
                st.warning("í‘œì‹œí•  ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        with tab7:
            st.markdown("###### ëª¨ë“  ì»¬ëŸ¼ë“¤ì˜ í†µê³„ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
            df = ff_df.reset_index(drop=True)
            st.dataframe(data=df, width=1400, height=600,hide_index=True)
    else:
        st.warning("Data Quality ë¶„ì„ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    return True    

def file_detail_analysis(df_ind, df_mapping):
    st.divider()
    st.markdown(f"### ğŸ“‘ íŒŒì¼ ìƒì„¸ ì •ë³´")
    
    final_files = sorted(df_ind['FileName'].unique())
    selected_file = st.selectbox("ì¡°íšŒí•  íŒŒì¼ì„ ìµœì¢… ì„ íƒí•˜ì„¸ìš”", final_files)

    if selected_file and df_mapping is not None:
        detail_df = df_mapping[df_mapping['FileName'] == selected_file]
        
        if not detail_df.empty:
            # ë©”íŠ¸ë¦­ í‘œì‹œ
            m1, m2, m3, m4 = st.columns(4)
            
            # ì´ ë ˆì½”ë“œ ìˆ˜ (ì—¬ëŸ¬ ì»¬ëŸ¼ëª… ì‹œë„)
            total_records = "N/A"
            try:
                if 'TotalRecords' in detail_df.columns:
                    val = detail_df['TotalRecords'].iloc[0]
                    if pd.notna(val):
                        total_records = f"{int(val):,}"
                elif 'RecordCnt' in detail_df.columns:
                    val = detail_df['RecordCnt'].iloc[0]
                    if pd.notna(val):
                        total_records = f"{int(val):,}"
                elif 'ValueCnt' in detail_df.columns:
                    val = detail_df['ValueCnt'].max()
                    if pd.notna(val):
                        total_records = f"{int(val):,}"
            except (ValueError, TypeError, IndexError, KeyError):
                pass
            
            # Sampling Row ìˆ˜
            sampling_row = "N/A"
            if 'SampleRows' in detail_df.columns:
                try:
                    # ìˆ«ìë¡œ ë³€í™˜ ì‹œë„ (NaN ì²˜ë¦¬ í¬í•¨)
                    sample_rows_series = pd.to_numeric(detail_df['SampleRows'], errors='coerce')
                    # NaNì´ ì•„ë‹Œ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                    valid_value = sample_rows_series.dropna()
                    if not valid_value.empty:
                        sampling_row = f"{int(valid_value.iloc[0]):,}"
                    # ëª¨ë“  í–‰ì´ NaNì¸ ê²½ìš°, ì²« ë²ˆì§¸ í–‰ì˜ ì›ë³¸ ê°’ í™•ì¸
                    elif not detail_df['SampleRows'].empty:
                        first_val = detail_df['SampleRows'].iloc[0]
                        if pd.notna(first_val) and str(first_val).strip() != '':
                            try:
                                sampling_row = f"{int(float(str(first_val))):,}"
                            except (ValueError, TypeError):
                                pass
                except Exception:
                    pass

            # Null(%) > 0% ì¸ ì»¬ëŸ¼ ìˆ˜
            null_0_cnt = "N/A"
            if 'Null(%)' in detail_df.columns:
                try:
                    null_pct_series = pd.to_numeric(detail_df['Null(%)'], errors='coerce')
                    null_0_cnt = f"{len(detail_df[null_pct_series > 0])}"
                except Exception:
                    pass

            # Null(%) == 100% ì¸ ì»¬ëŸ¼ ìˆ˜
            null_100_cnt = "N/A"
            if 'Null(%)' in detail_df.columns:
                try:
                    null_pct_series = pd.to_numeric(detail_df['Null(%)'], errors='coerce')
                    null_100_cnt = f"{len(detail_df[null_pct_series == 100])}"
                except Exception:
                    pass

            # Unique(%) == 100% ì¸ ì»¬ëŸ¼ ìˆ˜
            unique_100_cnt = "N/A"
            if 'Unique(%)' in detail_df.columns:
                try:
                    unique_pct_series = pd.to_numeric(detail_df['Unique(%)'], errors='coerce')
                    unique_100_cnt = f"{len(detail_df[unique_pct_series == 100])}"
                except Exception:
                    pass
            

            # ë©”íŠ¸ë¦­ í‘œì‹œ
            summary = {
                "Total Records": total_records,
                "Column #": len(detail_df),
                "Sampling #": sampling_row,
                "Has Null": null_0_cnt,
                "Has All Null": null_100_cnt,
                "Unique Columns": unique_100_cnt,
            }

            metric_colors = {
                "Total Records": "#1f77b4",      # íŒŒë€ìƒ‰ (ì •ë³´ì„±)
                "Column #": "#2ca02c",           # ì´ˆë¡ìƒ‰ (ê¸ì •ì )
                "Sampling #": "#9467bd",         # ë³´ë¼ìƒ‰ (ì •ë³´ì„±)
                "Has Null": "#ffbb78",           # ì—°í•œ ì£¼í™©ìƒ‰ (ê²½ê³ )
                "Has All Null": "#d62728",       # ë¹¨ê°„ìƒ‰ (ìœ„í—˜)
                "Unique Columns": "#17becf",     # ì²­ë¡ìƒ‰ (ê¸ì •ì )
            }

            cols = st.columns(len(summary))
            for col, (key, value) in zip(cols, summary.items()):
                color = metric_colors.get(key, "#0072B2") # ê¸°ë³¸ ìƒ‰ìƒ
                col.markdown(create_metric_card(value, key, color), unsafe_allow_html=True)

            result = Display_MasterFormat_Detail(detail_df)

        else:
            st.warning("ìƒì„¸ ë§¤í•‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("System íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

def main():
    st.title(APP_NAME)
    st.markdown(APP_DESC)

    # ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬
    df_vc, df_sys, df_vc_sys, df_mapping = load_data_validation()
    
    if df_vc is None or df_sys is None or df_vc_sys is None or df_mapping is None:
        st.error(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {VC_FILE}, {SYS_FILE}, {VC_SYS_FILE}, {MAPPING_FILE}")
        return

    selected_industry, df_ind = select_industry(df_vc, df_sys, df_vc_sys)

    all_activities = sorted(df_ind['Activity'].unique())
    all_systems = sorted(df_ind['System'].unique())

    # 2. Activity ì„¹ì…˜ (íŒŒì´ ì°¨íŠ¸ + ë…ë¦½ ì •ë³´)
    activity_analysis(df_ind, df_mapping, all_activities)

    # 3. System ì„¹ì…˜ (íŒŒì´ ì°¨íŠ¸ + ë…ë¦½ ì •ë³´)
    system_analysis(df_ind, df_mapping, all_systems)

    # 4. íŒŒì¼ ìƒì„¸ ì •ë³´ ì¶œë ¥
    file_detail_analysis(df_ind, df_mapping)

    # ë“œë¦´ë‹¤ìš´: íŠ¹ì • ì»¬ëŸ¼ì˜ Format ë¶„í¬ ë“±ì„ ë” ë³´ê³  ì‹¶ì„ ë•Œë¥¼ ìœ„í•œ í™•ì¥
    with st.expander("Raw Data (CodeMapping) ì „ì²´ ë³´ê¸°", expanded=False):
        st.dataframe(df_mapping, width="stretch", height=600, hide_index=True)
    st.divider()

if __name__ == "__main__":
    main()