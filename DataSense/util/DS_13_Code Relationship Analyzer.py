# -*- coding: utf-8 -*-
# DS_13_Code Relationship Analyzer.py
# ì½”ë“œ ê´€ê³„ ë¶„ì„ í”„ë¡œê·¸ë¨ì€ íŒŒì¼ í˜•ì‹ ë§¤í•‘ ê²°ê³¼ì™€ ë£° ë§¤í•‘ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œ ê´€ê³„ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
# 2025.12.24 Qliker

import pandas as pd
import numpy as np
import os
import sys
import logging

from pathlib import Path
import traceback
from typing import Dict, Any, Iterable, Optional, Sequence, List
from multiprocessing import Pool, cpu_count, Manager
from itertools import combinations

# ---------------------- ì „ì—­ ê¸°ë³¸ê°’ ----------------------
DEBUG_MODE = True   # ë””ë²„ê·¸ ëª¨ë“œ ì—¬ë¶€ (True: ë””ë²„ê·¸ ëª¨ë“œ, False: ìš´ì˜ ëª¨ë“œ)

OUTPUT_FILE_NAME = 'CodeMapping'       # ì½”ë“œ ê´€ê³„ ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì´ë¦„
OUTPUT_FILEFORMAT = 'FileFormatMapping' # íŒŒì¼ í˜•ì‹ ë§¤í•‘ ê²°ê³¼ íŒŒì¼ ì´ë¦„
OUTPUT_FILENUMERIC = 'FileNumericStats' # ìˆ«ì í˜•ì‹ í†µê³„ ê²°ê³¼ íŒŒì¼ ì´ë¦„

MATCH_RATE_THRESHOLD = 20 # ë§¤í•‘ ê²°ê³¼ ì¤‘ MatchRate(%) 20% ì´ìƒì¸ ë ˆì½”ë“œë§Œ ì„ íƒ (ê¸°ë³¸ê°’: 20%)

# ---------------------- í•¨ìˆ˜ ì„ ì–¸ ----------------------
def _clean_headers(df: pd.DataFrame) -> pd.DataFrame:
    """
    ë°ì´í„°í”„ë ˆì„ì˜ í—¤ë”ë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    out = df.copy()
    out.columns = [str(c).replace('\ufeff', '').strip() for c in out.columns]
    return out

# ---------------------------------------------------------
# 1. ì›Œì»¤ í•¨ìˆ˜ (CPU ì½”ì–´ë³„ë¡œ ë…ë¦½ ì‹¤í–‰ë˜ëŠ” ë¹„êµ ë¡œì§) internal mapping ì—ì„œ ì‚¬ìš©
# ---------------------------------------------------------
def compare_columns_worker(task_info):
    """
    ê° ì½”ì–´ì—ì„œ ì‹¤í–‰ë  ë…ë¦½ì ì¸ ë¹„êµ í•¨ìˆ˜
    task_info: (a_meta, b_meta, a_set, b_set)
    """
    a, b, a_set, b_set = task_info
    
    if not a_set or not b_set:
        return None

    # êµì§‘í•© ì—°ì‚° (Set ì—°ì‚°ì€ íŒŒì´ì¬ì—ì„œ ê°€ì¥ ë¹ ë¦„)
    intersection = a_set.intersection(b_set)
    compare_count = len(intersection)
    total_count = len(a_set)
    
    match_rate = round(compare_count / total_count * 100, 2) if total_count > 0 else 0.0

    # ì„ê³„ì¹˜(ì˜ˆ: 10%) ë¯¸ë§Œì€ ê²°ê³¼ì—ì„œ ì œì™¸í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
    if match_rate < 10.0:
        return None

    return {
        "FilePath": a['FilePath'], "FileName": a['FileName'], "ColumnName": a['ColumnName'],
        "MasterType": "Internal",
        "MasterFilePath": b['FilePath'], "MasterFile": b['FileName'],
        "ReferenceMasterType": "Internal", "MasterColumn": b['ColumnName'],
        "CompareLength": a.get('CompareLength', 0),
        "CompareCount": compare_count, "SourceCount": total_count, "MatchRate(%)": match_rate
    }

#-----------------------------------------------------------------------------------------------------    
def Expand_Format(Source_df, Mode='Reference') -> pd.DataFrame:
    """    Source_dfì˜ ìƒìœ„ 3ê°œ í¬ë§·(Format_1~3)ì„ í–‰ìœ¼ë¡œ í¼ì³ì„œ ë°˜í™˜.    """
    try:

        # 1) ì „ì²˜ë¦¬
        s_df = Source_df.copy().rename(columns={
            'Format':        'Format_1',
            'Format2nd':     'Format_2',
            'Format3rd':     'Format_3',
            'FormatMin':     'FormatMin_1',
            'FormatMax':     'FormatMax_1',
            'FormatMedian':   'FormatMedian_1',
            'Format2ndMin':  'FormatMin_2',
            'Format2ndMax':  'FormatMax_2',
            'Format2ndMedian': 'FormatMedian_2',
            'Format3rdMin':  'FormatMin_3',
            'Format3rdMax':  'FormatMax_3',
            'Format3rdMedian': 'FormatMedian_3',
            'FormatValue':   'FormatValue_1',
            'Format2ndValue':'FormatValue_2',
            'Format3rdValue':'FormatValue_3',
            'Format(%)':     'Format(%)_1',
            'Format2nd(%)':  'Format(%)_2',
            'Format3rd(%)':  'Format(%)_3',
        })

        # 2) i=1..3 ë³„ë¡œ ë¶„ë¦¬ â†’ í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ í†µì¼ â†’ ë¶™ì´ê¸°
        frames = []
        for i in (1, 2, 3):
            cols_i = [
                'FilePath', 'FileName', 'ColumnName', 'DetailDataType',  'MasterType', 'CompareLength', 'FormatCnt', 'UniqueCnt',
                f'Format_{i}', f'FormatMin_{i}', f'FormatMax_{i}', f'FormatMedian_{i}', 
                f'FormatValue_{i}', f'Format(%)_{i}'
            ]

            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
            available_cols = [c for c in cols_i if c in s_df.columns]
            if not available_cols:
                continue
                
            df_i = s_df[available_cols].copy()
            
            # ì»¬ëŸ¼ëª… ë³€ê²½ (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)
            rename_dict = {}
            if f'Format_{i}' in df_i.columns:
                rename_dict[f'Format_{i}'] = 'Format'
            if f'FormatMin_{i}' in df_i.columns:
                rename_dict[f'FormatMin_{i}'] = 'FormatMin'
            if f'FormatMax_{i}' in df_i.columns:
                rename_dict[f'FormatMax_{i}'] = 'FormatMax'
            if f'FormatMedian_{i}' in df_i.columns:
                rename_dict[f'FormatMedian_{i}'] = 'FormatMedian'
            if f'FormatValue_{i}' in df_i.columns:
                rename_dict[f'FormatValue_{i}'] = 'FormatValue'
            if f'Format(%)_{i}' in df_i.columns:
                rename_dict[f'Format(%)_{i}'] = 'Format(%)'
            
            if rename_dict:
                df_i = df_i.rename(columns=rename_dict)

            df_i['MatchNo'] = i

            # ë¹ˆ/ê²°ì¸¡ í¬ë§· ì œê±° (Format ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ë§Œ)
            if 'Format' in df_i.columns and not df_i.empty:
                format_series = df_i['Format']
                # Seriesì¸ì§€ í™•ì¸ (ë‹¨ì¼ ì»¬ëŸ¼ ì„ íƒì€ í•­ìƒ Series ë°˜í™˜)
                if isinstance(format_series, pd.Series):
                    mask = format_series.notna() & (format_series.astype(str).str.strip() != '')
                    df_i = df_i[mask]
                # ì´ìƒ ì¼€ì´ìŠ¤: DataFrameì´ ë°˜í™˜ëœ ê²½ìš°ëŠ” ìŠ¤í‚µ
                elif isinstance(format_series, pd.DataFrame):
                    continue

            # ìˆ«ìí˜• ì •ë¦¬
            for col in ('FormatValue', 'Format(%)', 'CompareLength'):
                if col in df_i.columns:
                    df_i[col] = pd.to_numeric(df_i[col], errors='coerce')

            frames.append(df_i)

        if not frames:
            return pd.DataFrame(columns=[
                'FilePath', 'FileName', 'ColumnName', 'DetailDataType', 'MasterType', 'FormatCnt', 'UniqueCnt', 'MatchNo',
                'Format', 'FormatMin', 'FormatMax', 'FormatMedian', 'FormatValue', 'Format(%)', 'CompareLength'
            ])

        result_df = pd.concat(frames, ignore_index=True)

        # ì •ë ¬ & ì¤‘ë³µ ì œê±°(ì„ íƒ)
        result_df = (result_df
                     .drop_duplicates()
                     .sort_values(['FilePath','FileName','ColumnName','MasterType','Format(%)'], ascending=[True, True, True, True, False])
                     .reset_index(drop=True))

        return result_df

    except Exception as e:
        print(f"ì „ì²´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


def Combine_Format(source_df, reference_df):
    """
    source_dfì™€ reference_dfë¥¼ ì¡°í•©í•˜ì—¬ ë°˜í™˜ (ê¸°ìˆ ì  ìµœì í™” ë²„ì „)
    """
    try:
        # ì œì™¸í•  íƒ€ì… ë¦¬ìŠ¤íŠ¸ (setìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ ì†ë„ í–¥ìƒ)
        except_types = {
            'Time', 'Timestamp', 'Date', 'DateTime', 'DATECHAR', 'TIME', 'TIMESTAMP', 
            'DATE', 'DATETIME', 'YEAR', 'YEARMONTH', 'YYMMDD', 'LATITUDE', 'LONGITUDE', 
            'TEL', 'CELLPHONE', 'ADDRESS', 'Alpha_Flag', 'Num_Flag', 'YN_Flag', 
            'NUM_Flag', 'KOR_Flag', 'KOR_Name'
        }

        # 1. í•„í„°ë§ ìµœì í™”: ë¶ˆí•„ìš”í•œ copy()ë¥¼ ì¤„ì´ê³  í•„í„°ë§ í›„ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        if 'DetailDataType' in source_df.columns:
            s_df = source_df[~source_df['DetailDataType'].isin(except_types)]
        else:
            s_df = source_df

        if 'DetailDataType' in reference_df.columns:
            r_df = reference_df[~reference_df['DetailDataType'].isin(except_types)]
        else:
            r_df = reference_df

        # 2. Merge ì „ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ ë° Rename (ë©”ëª¨ë¦¬ ì ˆì•½)
        rename_map = {
            'FilePath': 'MasterFilePath', 'FileName': 'MasterFile',
            'MasterType': 'ReferenceMasterType', 'ColumnName': 'MasterColumn',
            'FormatCnt': 'MasterFormatCnt', 'FormatMin': 'MasterMin',
            'FormatMax': 'MasterMax', 'FormatMedian': 'MasterMedian',
            'FormatValue': 'MasterValue', 'Format(%)': 'Master(%)',
            'UniqueCnt': 'MasterUniqueCnt'
        }
        
        # r_dfì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ê³¨ë¼ë‚´ë©° ë°”ë¡œ ì´ë¦„ì„ ë°”ê¿‰ë‹ˆë‹¤.
        r_cols = ['Format'] + list(rename_map.keys())
        r_df = r_df[r_cols].rename(columns=rename_map)

        # 3. Merge ë° ì¤‘ë³µ ì œê±°
        result_df = pd.merge(s_df, r_df, on='Format', how='left')
        result_df = result_df.dropna(subset=['MasterFile'])
        result_df = result_df.drop_duplicates(['FilePath', 'FileName', 'ColumnName', 'MasterFile', 'MasterColumn'])

        # 4. ìˆ«ìí˜• ë³€í™˜ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Vectorized fillna)
        num_cols = [
            'FormatCnt', 'FormatValue', 'UniqueCnt', 'CompareLength',
            'MasterFormatCnt', 'MasterValue', 'MasterUniqueCnt'
        ]
        for c in num_cols:
            if c in result_df.columns:
                result_df[c] = pd.to_numeric(result_df[c], errors='coerce').fillna(0)

        # 5. MasterCompareLength ê³„ì‚° ìµœì í™”
        # .str ì—°ì‚°ì€ ë¬´ê±°ìš°ë¯€ë¡œ ê²°ì¸¡ì¹˜ ë¨¼ì € ì²˜ë¦¬ í›„ ë§ˆì§€ë§‰ ê¸€ì ì¶”ì¶œ
        mcol_series = result_df['MasterColumn'].astype(str)
        last_char = mcol_series.str[-1]
        result_df['MasterCompareLength'] = np.where(last_char.str.isdigit(), last_char, '0')

        # 6. í”Œë˜ê·¸ ê³„ì‚° (ë¶ˆí•„ìš”í•œ Series ìƒì„±ì„ í”¼í•˜ê³  numpy ì—°ì‚° í™œìš©)
        # result_df['Format']ì´ objectì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ str.len() ì—°ì‚° ìµœì í™”
        fmt_len = result_df['Format'].astype(str).str.len()

        f0 = result_df['FormatMedian'].between(result_df['MasterMin'], result_df['MasterMax'])
        f1 = fmt_len > 1
        f2 = result_df['FormatCnt'] < result_df['MasterValue']
        f3 = ~( (result_df['FormatCnt'] >= result_df['MasterFormatCnt'] * 1.5) & 
                (result_df['FormatCnt'] >= 5) & 
                (result_df['MasterCompareLength'] == '0') )
        f5 = result_df['UniqueCnt'] >= 10
        f6 = result_df['FormatValue'] >= 10
        f8 = ~( (result_df['FilePath'] == result_df['MasterFilePath']) & 
                (result_df['FileName'] == result_df['MasterFile']) & 
                (result_df['ColumnName'] == result_df['MasterColumn']) )

        # 7. ìµœì¢… ê²°ê³¼ í• ë‹¹ (boolì„ intë¡œ ë°”ë¡œ ë³€í™˜)
        result_df['Match_Flag']  = f0.astype(int)
        result_df['Match_Flag1'] = f1.astype(int)
        result_df['Match_Flag2'] = f2.astype(int)
        result_df['Match_Flag3'] = f3.astype(int)
        result_df['Match_Flag4'] = 1
        result_df['Match_Flag5'] = f5.astype(int)
        result_df['Match_Flag6'] = f6.astype(int)
        result_df['Match_Flag7'] = 1
        result_df['Match_Flag8'] = f8.astype(int)

        # Final_Flag ì—°ì‚° (ë…¼ë¦¬ ì—°ì‚° & ê°€ ì‚°ìˆ  ê³±ì…ˆë³´ë‹¤ ë¹ ë¦„)
        # ëª¨ë“  í”Œë˜ê·¸ê°€ 1ì´ì–´ì•¼ í•˜ë¯€ë¡œ & ì—°ì‚°ìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        result_df['Final_Flag'] = (
            result_df['Match_Flag'] & result_df['Match_Flag2'] & 
            result_df['Match_Flag3'] & result_df['Match_Flag5'] & 
            result_df['Match_Flag6'] & result_df['Match_Flag8']
        ).astype(int)

        return result_df

    except Exception as e:
        print(f"ì¡°í•© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def Combine_Format_old(source_df, reference_df):
    """    source_dfì™€ reference_dfë¥¼ ì¡°í•©í•˜ì—¬ ë°˜í™˜.    """
    try:
        s_df = source_df.copy()

        except_detail_data_types = ['Time', 'Timestamp', 'Date', 'DateTime', 'DATECHAR', 'TIME', 'TIMESTAMP', 
            'DATE', 'DATETIME', 'YEAR', 'YEARMONTH', 'YYMMDD', 'LATITUDE', 'LONGITUDE', 'TEL', 'CELLPHONE', 'ADDRESS',
            'Alpha_Flag', 'Num_Flag', 'YN_Flag', 'NUM_Flag', 'KOR_Flag', 'KOR_Name']

        # DetailDataType ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ë§Œ í•„í„°ë§
        if 'DetailDataType' in s_df.columns:
            s_df = s_df[~s_df['DetailDataType'].isin(except_detail_data_types)].copy()
        r_df = reference_df.copy()
        if 'DetailDataType' in r_df.columns:
            r_df = r_df[~r_df['DetailDataType'].isin(except_detail_data_types)].copy()

        r_df = r_df[['FilePath', 'FileName', 'MasterType', 'ColumnName', 'FormatCnt', 'Format', 'FormatMin', 'FormatMax', 
                     'FormatMedian', 'FormatValue', 'Format(%)', 'UniqueCnt']].copy().rename(columns={
            'FilePath': 'MasterFilePath',
            'FileName': 'MasterFile',
            'MasterType': 'ReferenceMasterType',
            'ColumnName': 'MasterColumn',
            'FormatCnt': 'MasterFormatCnt',
            'Format': 'Format',
            'FormatMin': 'MasterMin',
            'FormatMax': 'MasterMax',
            'FormatMedian': 'MasterMedian',
            'FormatValue': 'MasterValue', 
            'Format(%)': 'Master(%)',
            'UniqueCnt': 'MasterUniqueCnt',
            # 'CompareLength': 'MasterCompareLength',
        })

        result_df = pd.merge(s_df, r_df, on=['Format'], how='left')
        result_df = result_df[result_df['MasterFile'].notna()]
        result_df = result_df.drop_duplicates(['FilePath','FileName','ColumnName','MasterFile','MasterColumn'])

        # --- ìˆ«ìí˜• ì»¬ëŸ¼ ì¼ê´„ ë³€í™˜ ---
        num_cols = [
            'FormatCnt','FormatValue','UniqueCnt','CompareLength',
            'MasterFormatCnt','MasterValue','MasterUniqueCnt'
        ]
        for c in num_cols:
            if c in result_df.columns:
                result_df[c] = pd.to_numeric(result_df[c], errors='coerce')

                # ê²°ì¸¡ ê¸°ë³¸ê°’ (ë¹„êµ ì•ˆì „ìš©)
        result_df['FormatCnt']        = result_df['FormatCnt'].fillna(0)
        result_df['FormatValue']      = result_df['FormatValue'].fillna(0)
        result_df['UniqueCnt']        = result_df['UniqueCnt'].fillna(0)
        result_df['MasterFormatCnt']  = result_df['MasterFormatCnt'].fillna(0)
        result_df['MasterValue']      = result_df['MasterValue'].fillna(0)
        result_df['MasterUniqueCnt']  = result_df['MasterUniqueCnt'].fillna(0)

        # --- MasterCompareLength ê³„ì‚° (MasterColumn ëìë¦¬ê°€ ìˆ«ìë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ "0") ---
        mcol_str = result_df['MasterColumn'].astype(str)
        last_char = mcol_str.str[-1].fillna('')
        result_df['MasterCompareLength'] = np.where(last_char.str.isdigit(), last_char, '0')

        # --- í”Œë˜ê·¸ ê³„ì‚°(ë²¡í„°í™”) ---
        # 0) í¬ë§· ì¤‘ì•™ê°’ì´ ë§ˆìŠ¤í„° ë²”ìœ„ ì•ˆì— ìˆëŠ”ê°€
        flag0 = result_df['FormatMedian'].ge(result_df['MasterMin']) & result_df['FormatMedian'].le(result_df['MasterMax'])

        # 1) í¬ë§· ê¸¸ì´ê°€ 1 ì´ˆê³¼ì¸ê°€ (ë¬¸ìì—´ ê¸¸ì´ ê¸°ì¤€)
        flag1 = result_df['Format'].astype(str).str.len().gt(1)

        # 2) ì†ŒìŠ¤ í¬ë§· ì¹´ìš´íŠ¸ê°€ ë§ˆìŠ¤í„° ê¸°ì¤€ê°’ë³´ë‹¤ ì‘ì€ê°€ (ì‘ì•„ì•¼ 1)
        flag2 = result_df['FormatCnt'].lt(result_df['MasterValue'])

        # 3) ê³¼ë„í•œ í¬ë§· ì¹´ìš´íŠ¸(=ë§ˆìŠ¤í„° 1.5ë°° ì´ìƒ & 5 ì´ìƒ)ì¸ë° MasterCompareLengthê°€ 0ì´ë©´ íƒˆë½
        flag3 = ~((result_df['FormatCnt'] >= result_df['MasterFormatCnt']*1.5) & (result_df['FormatCnt'] >= 5) & (result_df['MasterCompareLength'] == '0') )
 
        flag4 = pd.Series(True, index=result_df.index)  # 4) í•­ìƒ 1 (ìœ ì§€)
 
        flag5 = result_df['UniqueCnt'].ge(10)  # 5) ìœ ë‹ˆí¬ê°€ 10 ë¯¸ë§Œì´ë©´ íƒˆë½

        flag6 = result_df['FormatValue'].ge(10)  # 6) í¬ë§·ê°’ì´ 10 ë¯¸ë§Œì´ë©´ íƒˆë½

        # 7) ì†ŒìŠ¤ ìœ ë‹ˆí¬ê°€ ë§ˆìŠ¤í„°ë³´ë‹¤ í¬ë©´ì„œ MasterCompareLengthê°€ 0ì´ë©´ íƒˆë½
        flag7 = ~( (result_df['UniqueCnt'] > result_df['MasterUniqueCnt']) & (result_df['MasterCompareLength'] == '0') )

        # 8) FilePath = MasterFilePath ì´ê³  FileName = MasterFile ì´ê³  ColumnName = MasterColumn ì´ë©´ íƒˆë½
        flag8 = ~( (result_df['FilePath'] == result_df['MasterFilePath']) & (result_df['FileName'] == result_df['MasterFile']) & (result_df['ColumnName'] == result_df['MasterColumn']) )

        # ìµœì¢… í”Œë˜ê·¸
        result_df['Match_Flag']  = flag0.astype(int)
        result_df['Match_Flag1'] = flag1.astype(int)
        result_df['Match_Flag2'] = flag2.astype(int)
        result_df['Match_Flag3'] = flag3.astype(int)
        result_df['Match_Flag4'] = flag4.astype(int)
        result_df['Match_Flag5'] = flag5.astype(int)
        result_df['Match_Flag6'] = flag6.astype(int)
        # result_df['Match_Flag7'] = flag7.astype(int)
        result_df['Match_Flag7'] = 1
        result_df['Match_Flag8'] = flag8.astype(int)
        result_df['Final_Flag'] = (
            result_df['Match_Flag']  *
            result_df['Match_Flag2'] *
            result_df['Match_Flag3'] *
            result_df['Match_Flag4'] *
            result_df['Match_Flag5'] *
            result_df['Match_Flag6'] *
            result_df['Match_Flag7'] *
            result_df['Match_Flag8']
        )

        return result_df

    except Exception as e:
        print(f"ì¡°í•© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
#--------------[ í´ë˜ìŠ¤ ì„ ì–¸ ]--------------
# --- [1. ê²½ë¡œ ë° ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤] ---
class DQConfig:
    ROOT_PATH = Path(__file__).resolve().parents[2]
    YAML_RELATIVE_PATH = 'DataSense/util/DS_Master.yaml'
    # CONTRACT_RELATIVE_PATH = 'DataSense/util/DQ_Contract.yaml'

    @staticmethod
    def get_path(rel_path):
        """EXE ë¹Œë“œ í™˜ê²½ê³¼ ì¼ë°˜ íŒŒì´ì¬ í™˜ê²½ ëª¨ë‘ ëŒ€ì‘"""
        if hasattr(sys, '_MEIPASS'):
            return os.path.join(sys._MEIPASS, rel_path)
        return os.path.join(DQConfig.ROOT_PATH, rel_path)

# sys.path ì¶”ê°€ (ë‚´ë¶€ ëª¨ë“ˆ ì°¸ì¡°ìš©)
if str(DQConfig.ROOT_PATH) not in sys.path:
    sys.path.insert(0, str(DQConfig.ROOT_PATH))

try:
    from DataSense.util.io import Load_Yaml_File
    # from DataSense.util.dq_format import Expand_Format, Combine_Format
    from DataSense.util.dq_validate import (
        init_reference_globals,
        validate_date, validate_yearmonth, validate_latitude, validate_longitude,
        validate_YYMMDD, validate_year, validate_tel, validate_cellphone,
        validate_url, validate_email, validate_kor_name, validate_address,
        validate_country_code, validate_gender, validate_gender_en, validate_car_number,
        validate_time, validate_timestamp,
    )

except ImportError as e:
    print(f"í•„ìˆ˜ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    sys.exit(1)

class Initializing_Main_Class:
    def __init__(self, main_config):
        self.logger = self._setup_logger()
        self.config = main_config
        
    def _setup_logger(self):
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        return logging.getLogger(__name__)

    def process_files_mapping(self):
        try:
            # 1. íŒŒì¼ ë¡œë“œ (ìœ ì§€ë³´ìˆ˜ ìš©ì´í•˜ê²Œ ê²½ë¡œ ê´€ë¦¬)
            output_dir = Path(self.config['ROOT_PATH']) / self.config['directories']['output']
            meta_dir = Path(self.config['ROOT_PATH']) / "DataSense" / "DS_Meta"
            
            f_format_path = output_dir / "FileFormat.csv"
            r_datatype_path = output_dir / "RuleDataType.csv"
            m_meta_path = meta_dir / "Master_Meta.xlsx"

            # ë°ì´í„° ë¡œë“œ ì‹œ ì—ëŸ¬ ë°©ì§€ (str ì—°ì‚° ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ëª¨ë“  ì»¬ëŸ¼ì„ strë¡œ ì½ê±°ë‚˜ ì²˜ë¦¬)
            df_ff = pd.read_csv(f_format_path, encoding='utf-8-sig', dtype=str).fillna('')
            df_rt = pd.read_csv(r_datatype_path, encoding='utf-8-sig', dtype=str).fillna('')
            df_mm = pd.read_excel(m_meta_path, dtype=str).fillna('')

            self.logger.info("ëª¨ë“  ê¸°ë³¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")

            # 2.  'str' ì—°ì‚° ë¶€ë¶„ ìˆ˜ì • (Vectorized ì—°ì‚° ì‚¬ìš©)  ì˜ˆ: íŠ¹ì • ì»¬ëŸ¼ì—ë§Œ str ì—°ì‚° ì ìš©
            for df in [df_ff, df_rt, df_mm]:
                for col in df.columns:
                    # ë°ì´í„°í”„ë ˆì„ ìì²´ê°€ ì•„ë‹Œ, ê°œë³„ ì‹œë¦¬ì¦ˆ(ì»¬ëŸ¼)ì— str.strip() ì ìš©
                    df[col] = df[col].astype(str).str.strip()

            # 3. ì½”ë“œ ê´€ê³„ ë¶„ì„ ë©”ì¸ ë¡œì§ 
            result_df = self.execute_relationship_analysis(df_ff, df_rt, df_mm)
            
            final_path = output_dir / "Code_Relationship_Result.csv"
            result_df.to_csv(final_path, index=False, encoding='utf-8-sig')
            self.logger.info(f"ë¶„ì„ ì™„ë£Œ ë° ì €ì¥: {final_path}")
            
            return True

        except Exception as e:
            self.logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.logger.error(traceback.format_exc())
            return False

    def execute_relationship_analysis(self, df_ff, df_rt, df_mm) -> pd.DataFrame:
        """
        df_ff: íŒŒì¼ í˜•ì‹ ë§¤í•‘ ê²°ê³¼
        df_rt: ë£° ë§¤í•‘ ê²°ê³¼
        df_mm: ë§ˆìŠ¤í„° ë§¤í•‘ ê²°ê³¼
        """
        # 1) Reference
        reference_df = self.reference_mapping(df_ff)
        if DEBUG_MODE and reference_df is not None and not reference_df.empty:
            p = os.path.join(self.config['ROOT_PATH'], self.config['directories']['output'], OUTPUT_FILE_NAME + '_3rd_ref_mapping.csv')
            reference_df.to_csv(p, index=False, encoding='utf-8-sig')
            self.logger.info(f"reference mapping : {p} ì €ì¥")

        # 2) Rule
        rule_df = self.rule_mapping(df_ff, df_rt)
        if DEBUG_MODE and rule_df is not None and not rule_df.empty:
            p = os.path.join(self.config['ROOT_PATH'], self.config['directories']['output'], OUTPUT_FILE_NAME + '_4th_rule_mapping.csv')
            rule_df.to_csv(p, index=False, encoding='utf-8-sig')
            self.logger.info(f"rule mapping : {p} ì €ì¥")

        # 3) Numeric stats
        numeric_df = self.numeric_column_statistics(df_ff)
        if DEBUG_MODE and numeric_df is not None and not numeric_df.empty:
            p = os.path.join(self.config['ROOT_PATH'], self.config['directories']['output'], OUTPUT_FILENUMERIC + '.csv')
            numeric_df.to_csv(p, index=False, encoding='utf-8-sig')
            self.logger.info(f"numeric stats : {p} ì €ì¥")

        # 4) Internal
        internal_df = self.internal_mapping(df_ff) # "ë°ì´í„°ê°€ ìƒê¸´ ëª¨ì–‘(Pattern)ì´ ê°™ì€ ê²ƒë“¤ë¼ë¦¬ë§Œ" ê·¸ë£¹í•‘í•˜ì—¬ ë¹„êµ ëŒ€ìƒì„ í™• ì¤„ì—¬ë²„ë¦½ë‹ˆë‹¤.
        if DEBUG_MODE and internal_df is not None and not internal_df.empty:
            p = os.path.join(self.config['ROOT_PATH'], self.config['directories']['output'], OUTPUT_FILE_NAME + '_7th_int_mapping.csv')
            internal_df.to_csv(p, index=False, encoding='utf-8-sig')
            self.logger.info(f"internal mapping : {p} ì €ì¥")

        # # 4) Internal New () N X N ì¡°í•©ì„ ë§Œë“  ë’¤ í•˜ë‚˜ì”© ê²€ì‚¬í•˜ëŠ” ë°©ë²• (ì†ë„ê°€ ë” ëŠë¦¼) 
        # internal_df_new = self.internal_mapping_new(df_ff)
        # if DEBUG_MODE and internal_df_new is not None and not internal_df_new.empty:
        #     p = os.path.join(self.config['ROOT_PATH'], self.config['directories']['output'], OUTPUT_FILE_NAME + '_7th_int_mapping_new.csv')
        #     internal_df_new.to_csv(p, index=False, encoding='utf-8-sig')
        #     self.logger.info(f"internal mapping_new : {p} ì €ì¥")

        # 5) concat + pivot + final
        concat_df = self.mapping_concat(reference_df, internal_df, rule_df)
        if DEBUG_MODE and concat_df is not None and not concat_df.empty:
            p = os.path.join(self.config['ROOT_PATH'], self.config['directories']['output'], OUTPUT_FILE_NAME + '_8th_concat.csv')
            concat_df.to_csv(p, index=False, encoding='utf-8-sig')
            self.logger.info(f"concat_df : {p} ì €ì¥")

        pivoted_df = self.mapping_pivot(internal_df) 
        if DEBUG_MODE and pivoted_df is not None and not pivoted_df.empty:
            p = os.path.join(self.config['ROOT_PATH'], self.config['directories']['output'], OUTPUT_FILE_NAME + '_9th_pivoted.csv')
            pivoted_df.to_csv(p, index=False, encoding='utf-8-sig')
            self.logger.info(f"pivoted_df : {p} ì €ì¥")

        final_df = self.final_mapping(df_ff, pivoted_df, reference_df, rule_df) # ìƒˆë¡œìš´ ë°©ì‹ 
        if DEBUG_MODE and final_df is not None and not final_df.empty:
            final_path = os.path.join(self.config['ROOT_PATH'], self.config['directories']['output'], OUTPUT_FILE_NAME + '_final.csv')
            final_df.to_csv(final_path, index=False, encoding='utf-8-sig')
            self.logger.info(f"ìµœì¢… : {final_path} ì €ì¥")
           
        return final_df

# ---------------------------------------------------------
# 2. ë©”ì¸ í´ë˜ìŠ¤ ë‚´ í™•ì¥ ë©”ì„œë“œ 
# ---------------------------------------------------------
    def internal_mapping_new(self, fileformat_df: pd.DataFrame, sample_size: int = 10000):
        self.logger.info(f"ğŸš€ ì´ˆê³ ì† Internal Mapping ì‹œì‘ (ìƒ˜í”Œë§: {sample_size}ê±´)")
        
        # 1. ìœ ë‹ˆí¬ ì…‹ ì‚¬ì „ ì¶”ì¶œ (I/O ìµœì†Œí™”)
        unique_sets = {}
        target_cols = fileformat_df.to_dict('records')
        
        self.logger.info(f"ëŒ€ìƒ ì»¬ëŸ¼ {len(target_cols)}ê°œì— ëŒ€í•œ ë°ì´í„° ë¡œë”© ë° ìƒ˜í”Œë§ ì‹œì‘...")
        for col_meta in target_cols:
            fpath = col_meta['FilePath']
            cname = col_meta['ColumnName']
            
            try:
                # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ, ì§€ì •ëœ ìƒ˜í”Œë§Œí¼ ì½ê¸°
                df_tmp = pd.read_csv(fpath, usecols=[cname], dtype=str, encoding='utf-8-sig', low_memory=False)
                if len(df_tmp) > sample_size:
                    series = df_tmp[cname].sample(n=sample_size, random_state=42)
                else:
                    series = df_tmp[cname]
                
                # í´ë Œì§• í›„ Set ì €ì¥
                cleaned_set = set(series.dropna().str.strip().unique())
                unique_sets[(fpath, cname)] = cleaned_set
            except Exception:
                unique_sets[(fpath, cname)] = set()

        # 2. Pruning (ê°€ì§€ì¹˜ê¸°) ê¸°ë°˜ íƒœìŠ¤í¬ ìƒì„±
        self.logger.info("ë©”íƒ€ë°ì´í„° ê¸°ë°˜ Pruning(ê°€ì§€ì¹˜ê¸°) ìˆ˜í–‰ ì¤‘...")
        tasks = []
        for a, b in combinations(target_cols, 2):
            # [ì¡°ê±´ 1] ê°™ì€ íŒŒì¼ì˜ ê°™ì€ ì»¬ëŸ¼ì€ ì œì™¸
            if a['FilePath'] == b['FilePath'] and a['ColumnName'] == b['ColumnName']:
                continue
            
            # # [ì¡°ê±´ 2] ë°ì´í„° íƒ€ì…ì´ ë‹¤ë¥´ë©´ ì—°ì‚° ê°€ì¹˜ ì—†ìŒ (Pruning)
            # if a.get('DetailDataType') != b.get('DetailDataType'):
            #     continue
            
            # [ì¡°ê±´ 3] ê°’ ë²”ìœ„(Min/Max)ê°€ ì „í˜€ ê²¹ì¹˜ì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ (Pruning)
            a_min, a_max = str(a.get('FormatMin_1', '')), str(a.get('FormatMax_1', ''))
            b_min, b_max = str(b.get('FormatMin_1', '')), str(b.get('FormatMax_1', ''))
            
            if a_min and a_max and b_min and b_max:
                if a_max < b_min or a_min > b_max:
                    continue # ë²”ìœ„ê°€ ê²¹ì¹˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìŠ¤í‚µ

            # ë¹„êµ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
            set_a = unique_sets.get((a['FilePath'], a['ColumnName']), set())
            set_b = unique_sets.get((b['FilePath'], b['ColumnName']), set())
            
            if set_a and set_b:
                tasks.append((a, b, set_a, set_b))

        self.logger.info(f"ìµœì¢… ë¹„êµ ëŒ€ìƒ ì¡°í•©: {len(tasks)}ê°œ (ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘)")

        # 3. ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        with Pool(processes=cpu_count()) as pool:
            results = pool.map(compare_columns_worker, tasks)

        # 4. ê²°ê³¼ ì •ë¦¬
        final_results = [r for r in results if r is not None]
        self.logger.info(f"ë¶„ì„ ì™„ë£Œ! ìœ íš¨ ë§¤í•‘ ê²°ê³¼: {len(final_results)}ê±´")
        
        return pd.DataFrame(final_results)

# ------------------ (2) Reference ê°’ ë¹„êµ (new) ------------------
    def mapping_check_old(self, mapping_df: pd.DataFrame, sample: int = 10_000) -> pd.DataFrame:
        """Reference/Internal ë§¤í•‘ ë¹„êµ ìˆ˜í–‰ + í•„ìˆ˜ ì»¬ëŸ¼ ë³´ì¥"""
        
        def _clean_values(series: pd.Series, length_limit=0) -> pd.Series:
            s = (series.dropna().astype(str).str.strip()
                 .replace({'': pd.NA, 'nan': pd.NA, 'None': pd.NA})).dropna()
            if length_limit and length_limit > 0:
                s = s.str[:int(length_limit)]
            return s.drop_duplicates()

        def _to_int(x, default=0):
            try:
                v = pd.to_numeric(x, errors="coerce")
                return int(v) if pd.notna(v) else default
            except Exception:
                return default

        # âœ… 1. ìºì‹œ ì €ì¥ì†Œ ì´ˆê¸°í™” (ë£¨í”„ ë°–)
        master_val_cache = {}
        src_cache, master_cache = {}, {}
        rows: List[Dict[str, Any]] = []

        mapping_df = mapping_df.copy()

        for _, r in mapping_df.sort_values(by='FilePath').iterrows():
            # âœ… 2. ë£¨í”„ ë‚´ë¶€ì—ì„œ ë³€ìˆ˜ ì •ì˜
            fpath = str(r['FilePath']).strip()
            fname = str(r['FileName']).strip()
            col   = str(r['ColumnName']).strip()
            mtype = str(r['MasterType']).strip()
            mpath = str(r['MasterFilePath']).strip()
            mfile = str(r.get('MasterFile', "")).strip()
            rtype = str(r['ReferenceMasterType']).strip()
            mcol  = str(r['MasterColumn']).strip()

            comp_len_src = _to_int(r.get('CompareLength', 0), 0)
            comp_len_mst = _to_int(r.get('MasterCompareLength', 0), 0)

            # --- íŒŒì¼ ë¡œë“œ ë¡œì§ (ìƒëµ ë°©ì§€ìš© ìœ ì§€) ---
            if fpath not in src_cache:
                try:
                    src_cache[fpath] = _clean_headers(pd.read_csv(fpath, encoding='utf-8-sig', low_memory=False, dtype=str))
                except Exception: continue
            if mpath not in master_cache:
                try:
                    master_cache[mpath] = _clean_headers(pd.read_csv(mpath, encoding='utf-8-sig', low_memory=False, dtype=str))
                except Exception: continue

            df = src_cache[fpath]
            md = master_cache[mpath]

            if (col not in df.columns) or (mcol not in md.columns):
                continue

            # âœ… 3. ë§ˆìŠ¤í„° ê°’ ì¶”ì¶œ ë° ìºì‹± (ë³€ìˆ˜ê°€ ëª¨ë‘ ì •ì˜ëœ ë£¨í”„ ì•ˆì—ì„œ ìˆ˜í–‰)
            # ë§ˆìŠ¤í„° ì»¬ëŸ¼ê³¼ ì ìš©í•  ê¸¸ì´ ì œí•œì„ í‚¤ë¡œ ì‚¬ìš©
            m_key = (mpath, mcol, comp_len_mst or comp_len_src)
            if m_key not in master_val_cache:
                # ë§ˆìŠ¤í„° ë°ì´í„°ì…‹(md)ì—ì„œ í•´ë‹¹ ì»¬ëŸ¼(mcol)ì„ ê°€ì ¸ì™€ í´ë Œì§• í›„ ìºì‹œì— ì €ì¥
                master_val_cache[m_key] = _clean_values(md[mcol], m_key[2])
            
            m_vals = master_val_cache[m_key]

            # --- ì†ŒìŠ¤ ë°ì´í„° ìƒ˜í”Œë§ ë° ë¹„êµ ---
            s_series = df[col]
            if len(s_series) > sample:
                s_series = s_series.sample(sample, random_state=42)

            s_vals = _clean_values(s_series, comp_len_src or comp_len_mst)

            # ë¹„êµ ìˆ˜í–‰
            compare_count = s_vals[s_vals.isin(m_vals)].count()
            total_count   = s_vals.count()
            match_rate    = round(compare_count / total_count * 100, 2) if total_count > 0 else 0.0

            rows.append({
                "FilePath": fpath, "FileName": fname, "ColumnName": col, "MasterType": mtype,
                "MasterFilePath": mpath, "MasterFile": mfile, "ReferenceMasterType": rtype,
                "MasterColumn": mcol, "CompareLength": comp_len_src,
                "CompareCount": compare_count, "SourceCount": total_count, "MatchRate(%)": match_rate
            })

        out = pd.DataFrame(rows)
        return out
    
    def mapping_check(self, mapping_df: pd.DataFrame, sample: int = 10000) -> pd.DataFrame:
        """ê¸°ì¡´ íŒ¨í„´ ë§¤í•‘ ë°©ì‹ì„ ìœ ì§€í•˜ë˜, ë°ì´í„° ì •ì œ ê³¼ì •ì„ ìºì‹±í•˜ì—¬ ì†ë„ ìµœì í™”"""
        
        # --- ë‚´ë¶€ ìœ í‹¸ë¦¬í‹°: ê°’ì„ í•œ ë²ˆë§Œ ì •ì œí•´ì„œ ì €ì¥ ---
        cleaned_cache = {} # (fpath, col, limit) -> cleaned_series_set

        def get_cleaned_values(fpath, col, df_source, limit):
            key = (fpath, col, limit)
            if key not in cleaned_cache:
                # 1. ìƒ˜í”Œë§ ë° ì •ì œ (ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰)
                s = df_source[col].dropna().astype(str).str.strip()
                s = s.replace({'': pd.NA, 'nan': pd.NA, 'None': pd.NA}).dropna()
                
                if len(s) > sample:
                    s = s.sample(sample, random_state=42)
                
                if limit > 0:
                    s = s.str[:int(limit)]
                
                # êµì§‘í•© ì—°ì‚°ì„ ìœ„í•´ setìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ìºì‹±
                cleaned_cache[key] = set(s.unique())
            return cleaned_cache[key]

        # --------------------------------------------------
        mapping_df = mapping_df.copy()
        rows = []
        src_cache = {} # íŒŒì¼ ê°ì²´ ìºì‹œ

        # FilePath ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ íŒŒì¼ ë¡œë“œ íšŸìˆ˜ ìµœì†Œí™”
        for _, r in mapping_df.sort_values(by=['FilePath', 'MasterFilePath']).iterrows():
            fpath, col = str(r['FilePath']), str(r['ColumnName'])
            mpath, mcol = str(r['MasterFilePath']), str(r['MasterColumn'])
            
            # íŒŒì¼ ë¡œë“œ (ìºì‹œ í™œìš©)
            for path in [fpath, mpath]:
                if path not in src_cache:
                    try:
                        src_cache[path] = _clean_headers(pd.read_csv(path, dtype=str, encoding='utf-8-sig', low_memory=False))
                    except: continue

            if fpath not in src_cache or mpath not in src_cache: continue
            
            df, md = src_cache[fpath], src_cache[mpath]
            if col not in df.columns or mcol not in md.columns: continue

            # ë¹„êµ ê¸¸ì´ ì„¤ì •
            comp_len = int(r.get('CompareLength', 0) or r.get('MasterCompareLength', 0))

            # âœ… í•µì‹¬: ì´ë¯¸ ì •ì œëœ ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜´ (ì¤‘ë³µ ì—°ì‚° 0)
            s_vals_set = get_cleaned_values(fpath, col, df, comp_len)
            m_vals_set = get_cleaned_values(mpath, mcol, md, comp_len)

            # âœ… ê³ ì† Set êµì§‘í•© ì—°ì‚°
            intersection = s_vals_set.intersection(m_vals_set)
            compare_count = len(intersection)
            total_count = len(s_vals_set)
            match_rate = round(compare_count / total_count * 100, 2) if total_count > 0 else 0.0

            if match_rate >= MATCH_RATE_THRESHOLD:
                rows.append({
                    "FilePath": fpath, "FileName": r['FileName'], "ColumnName": col, 
                    "MasterType": r['MasterType'], "MasterFilePath": mpath, "MasterFile": r.get('MasterFile',''),
                    "ReferenceMasterType": r['ReferenceMasterType'], "MasterColumn": mcol,
                    "CompareLength": comp_len, "CompareCount": compare_count, 
                    "SourceCount": total_count, "MatchRate(%)": match_rate
                })

        return pd.DataFrame(rows)
    # # ------------------ (2) í”¼ë²—(Left-compact) ------------------
    # def mapping_pivot_old(self, df_merged: pd.DataFrame, valid_threshold: float = 10.0,
    #                   top_k: int = 3, drop_old_pivot_cols: bool = True) -> pd.DataFrame:
    #     """Left-compact pivot: ìƒìœ„ top_k í›„ë³´ë¥¼ CodeFilePath/CodeFile/CodeType/CodeColumn/Matchedë¡œ ì „ê°œ"""
    #     if df_merged is None or df_merged.empty:
    #         cols = ["FilePath","FileName","ColumnName","MasterType"]
    #         for b in ["CodeFilePath","CodeFile","CodeType","CodeColumn","Matched","Matched(%)"]:
    #             cols += [f"{b}_{i}" for i in range(1, top_k+1)]
    #         return pd.DataFrame(columns=cols)

    #     df = df_merged.copy()
    #     # normalize numeric columns
    #     for numc in ("Matched","Matched(%)"):
    #         if numc in df.columns:
    #             df[numc] = pd.to_numeric(df[numc], errors='coerce').fillna(0)

    #     # keep only candidate rows that exceed thresholds
    #     mask = (df["Matched"].fillna(0) > 0) & (df["Matched(%)"].fillna(-1) > valid_threshold)
    #     df = df.loc[mask].copy()
    #     if df.empty:
    #         cols = ["FilePath","FileName","ColumnName","MasterType"]
    #         for b in ["CodeFilePath","CodeFile","CodeType","CodeColumn","Matched","Matched(%)"]:
    #             cols += [f"{b}_{i}" for i in range(1, top_k+1)]
    #         return pd.DataFrame(columns=cols)

    #     sort_keys = ["FilePath","FileName","ColumnName","MasterType","Matched(%)","Matched"]
    #     df = df.sort_values(sort_keys, ascending=[True,True,True,True,False,False], kind="mergesort").reset_index(drop=True)

    #     grp_keys = ["FilePath","FileName","ColumnName","MasterType"]
    #     df = df.assign(rank=df.groupby(grp_keys).cumcount() + 1)
    #     df = df.loc[df["rank"] <= top_k].copy()

    #     wide = (
    #         df.pivot_table(
    #             index=grp_keys,
    #             columns="rank",
    #             values=["CodeFilePath","CodeFile","CodeType","CodeColumn","Matched","Matched(%)"],
    #             aggfunc="first"
    #         )
    #     )
    #     # Normalize column names to previous naming (CodeFile / CodeColumn)
    #     # pivot produced e.g. ('CodeFilePath', 1)
    #     wide.columns = [f"{col[0]}_{int(col[1])}" for col in wide.columns]
    #     wide = wide.reset_index().copy()

    #     # Left-compact each block of parallel columns
    #     def _left_compact_block(block: pd.DataFrame) -> pd.DataFrame:
    #         arr = block.to_numpy(object)
    #         for r in range(arr.shape[0]):
    #             vals = [x for x in arr[r].tolist() if not (pd.isna(x) or str(x).strip() == "")]
    #             vals += [""] * (arr.shape[1] - len(vals))
    #             arr[r, :] = vals
    #         return pd.DataFrame(arr, columns=block.columns, index=block.index)

    #     # perform left-compact for groups
    #     for base in ["CodeFilePath","CodeFile","CodeType","CodeColumn","Matched","Matched(%)"]:
    #         cols = [c for c in wide.columns if c.startswith(base + "_")]
    #         if cols:
    #             block = _left_compact_block(wide[cols].copy())
    #             wide[cols] = block

    #     # fillna -> empty string for object columns
    #     obj_cols = wide.select_dtypes(include="object").columns.tolist()
    #     if obj_cols:
    #         wide[obj_cols] = wide[obj_cols].fillna("")

    #     return wide

    # 2025. 12. 24 Qliker - í”¼ë²—(Left-compact) ìˆ˜ì •
    def mapping_pivot(self, df_merged: pd.DataFrame, valid_threshold: float = 10.0,
                         top_k: int = 3, drop_old_pivot_cols: bool = True) -> pd.DataFrame:
        """Top-K í›„ë³´ë¥¼ ê°€ë¡œë¡œ ì „ê°œí•˜ëŠ” ê°œì„ ëœ í”¼ë²— ë¡œì§"""
        
        if df_merged is None or df_merged.empty:
            return self._make_empty_pivot_df(top_k)

        # 1. ì»¬ëŸ¼ëª… ì •ë¦¬
        rename_map = {
            'MasterFilePath':'CodeFilePath', 'MasterFile':'CodeFile',
            'ReferenceMasterType':'CodeType', 'MasterColumn':'CodeColumn',
            'CompareCount':'Matched', 'MatchRate(%)':'Matched(%)'
        }
        df = df_merged.rename(columns=rename_map).copy()

        # 2. ìˆ«ìí˜• ë³€í™˜ ë° í•„í„°ë§
        for numc in ["Matched", "Matched(%)"]:
            if numc in df.columns:
                df[numc] = pd.to_numeric(df[numc], errors='coerce').fillna(0)

        mask = (df["Matched"] > 0) & (df["Matched(%)"] >= valid_threshold)
        df = df.loc[mask].copy()
        
        if df.empty:
            return self._make_empty_pivot_df(top_k)

        # 3. ì •ë ¬ ë° ë­í‚¹ ë¶€ì—¬ (Top-K ì¶”ì¶œ)
        grp_keys = ["FilePath", "FileName", "ColumnName", "MasterType"]
        sort_keys = grp_keys + ["Matched(%)", "Matched"]
        
        df = df.sort_values(sort_keys, ascending=[True]*4 + [False]*2, kind="mergesort")
        df['rank'] = df.groupby(grp_keys).cumcount() + 1
        df = df.loc[df["rank"] <= top_k]

        # 4. Pivot Table ìƒì„±
        value_vars = ["CodeFilePath", "CodeFile", "CodeType", "CodeColumn", "Matched", "Matched(%)"]
        wide = df.pivot_table(
            index=grp_keys,
            columns="rank",
            values=value_vars,
            aggfunc="first"
        )

        # 5. ì»¬ëŸ¼ëª… í‰íƒ„í™” (Multi-index -> Single-index) ì˜ˆ: ('CodeFile', 1) -> 'CodeFile_1'
        wide.columns = [f"{c[0]}_{int(c[1])}" for c in wide.columns]
        wide = wide.reset_index()

        # 6. ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ (CodeFile_1, Matched_1, CodeFile_2, Matched_2... ìˆœì„œë¡œ ì •ë ¬í•˜ê³  ì‹¶ì„ ë•Œ)
        ordered_cols = grp_keys.copy()
        for i in range(1, top_k + 1):
            for base in value_vars:
                col_name = f"{base}_{i}"
                if col_name in wide.columns:
                    ordered_cols.append(col_name)
                else:
                    wide[col_name] = "" if "Matched" not in base else 0
                    ordered_cols.append(col_name)

        return wide[ordered_cols].fillna("")

    def _make_empty_pivot_df(self, top_k):
        """ë¹ˆ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± ìœ í‹¸ë¦¬í‹°"""
        cols = ["FilePath", "FileName", "ColumnName", "MasterType"]
        bases = ["CodeFilePath", "CodeFile", "CodeType", "CodeColumn", "Matched", "Matched(%)"]
        for i in range(1, top_k + 1):
            for b in bases:
                cols.append(f"{b}_{i}")
        return pd.DataFrame(columns=cols)

    # ------------------ (2) í”¼ë²—(Left-compact) ------------------
    def mapping_pivot_old(self, df_merged: pd.DataFrame, valid_threshold: float = 10.0,
                      top_k: int = 3, drop_old_pivot_cols: bool = True) -> pd.DataFrame:
        """Left-compact pivot: ìƒìœ„ top_k í›„ë³´ë¥¼ CodeFilePath/CodeFile/CodeType/CodeColumn/Matchedë¡œ ì „ê°œ"""
        df_merged = df_merged.rename(columns={
            'MasterFilePath':'CodeFilePath',
            'MasterFile':'CodeFile',
            'ReferenceMasterType':'CodeType',
            'MasterColumn':'CodeColumn',
            'CompareLength':'CompareLength',
            'MatchRate(%)':'Matched(%)',
            'CompareCount':'Matched',
            'SourceCount':'SourceCount',
            'MatchRate(%)':'Matched(%)'
        })

        # df_merged = df_merged[df_merged['Matched(%)'] > MATCH_RATE_THRESHOLD]

        if df_merged is None or df_merged.empty:
            cols = ["FilePath","FileName","ColumnName","MasterType"]
            for b in ["CodeFilePath","CodeFile","CodeType","CodeColumn","Matched","Matched(%)"]:
                cols += [f"{b}_{i}" for i in range(1, top_k+1)]
            return pd.DataFrame(columns=cols)

        df = df_merged.copy()
        # normalize numeric columns
        for numc in ("Matched","Matched(%)"):
            if numc in df.columns:
                df[numc] = pd.to_numeric(df[numc], errors='coerce').fillna(0)

        # keep only candidate rows that exceed thresholds
        mask = (df["Matched"].fillna(0) > 0) & (df["Matched(%)"].fillna(-1) > valid_threshold)
        df = df.loc[mask].copy()
        if df.empty:
            cols = ["FilePath","FileName","ColumnName","MasterType"]
            for b in ["CodeFilePath","CodeFile","CodeType","CodeColumn","Matched","Matched(%)"]:
                cols += [f"{b}_{i}" for i in range(1, top_k+1)]
            return pd.DataFrame(columns=cols)

        sort_keys = ["FilePath","FileName","ColumnName","MasterType","Matched(%)","Matched"]
        df = df.sort_values(sort_keys, ascending=[True,True,True,True,False,False], kind="mergesort").reset_index(drop=True)

        grp_keys = ["FilePath","FileName","ColumnName","MasterType"]
        df = df.assign(rank=df.groupby(grp_keys).cumcount() + 1)
        df = df.loc[df["rank"] <= top_k].copy()

        wide = (
            df.pivot_table(
                index=grp_keys,
                columns="rank",
                values=["CodeFilePath","CodeFile","CodeType","CodeColumn","Matched","Matched(%)"],
                aggfunc="first"
            )
        )
        # Normalize column names to previous naming (CodeFile / CodeColumn)
        wide.columns = [f"{col[0]}_{int(col[1])}" for col in wide.columns]
        wide = wide.reset_index().copy()

        # Left-compact each block of parallel columns
        def _left_compact_block(block: pd.DataFrame) -> pd.DataFrame:
            arr = block.to_numpy(object)
            for r in range(arr.shape[0]):
                vals = [x for x in arr[r].tolist() if not (pd.isna(x) or str(x).strip() == "")]
                vals += [""] * (arr.shape[1] - len(vals))
                arr[r, :] = vals
            return pd.DataFrame(arr, columns=block.columns, index=block.index)

        # perform left-compact for groups
        for base in ["CodeFilePath","CodeFile","CodeType","CodeColumn","Matched","Matched(%)"]:
            cols = [c for c in wide.columns if c.startswith(base + "_")]
            if cols:
                block = _left_compact_block(wide[cols].copy())
                wide[cols] = block

        # fillna -> empty string for object columns
        obj_cols = wide.select_dtypes(include="object").columns.tolist()
        if obj_cols:
            wide[obj_cols] = wide[obj_cols].fillna("")

        return wide
    # ------------------ (3) Rule ë§¤í•‘ ------------------
    def rule_mapping(
        self,
        fileformat_df: pd.DataFrame,
        ruldatatype_df: pd.DataFrame,
        valid_types: Sequence[str] = (
            'URL','YEAR','EMAIL','CELLPHONE','TEL','LATITUDE','LONGITUDE',
            'DATECHAR','YEARMONTH','YYMMDD','ADDRESS','KOR_NAME', 'TIME', 'TIMESTAMP',
            'COUNTRY_ISO3','êµ­ê°€ì½”ë“œ','ì‹œë„','ì°¨ëŸ‰ë²ˆí˜¸','GENDER','GENDER_EN'
        ),
        encodings_try: Iterable[str] = ("utf-8-sig","utf-8","cp949"),
        sampling_rows: Optional[int] = None,
        use_valuecnt_fallback: bool = True
    ) -> pd.DataFrame:
        """
        ruldatatype_dfë¥¼ ë³´ê³  íŒŒì¼ì˜ ì»¬ëŸ¼ì— ëŒ€í•´ validate_* í•¨ìˆ˜ë¥¼ ì ìš©í•´ì„œ ë£° ê¸°ë°˜ ë§¤í•‘ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ ë°˜í™˜
        """
        out_cols = [
            'FilePath','FileName','ColumnName','MasterType','MasterColumn',
            'CompareCount','MatchRate(%)','MasterFile','ReferenceMasterType',
            'MasterFilePath','CompareLength','SourceCount'
        ]
        if ruldatatype_df is None or ruldatatype_df.empty:
            self.logger.info("[rule_mapping] ruldatatype_df is empty")
            return pd.DataFrame(columns=out_cols)

        required_cols = ["FilePath","FileName","ColumnName","MasterType","ValueCnt","Rule","MatchedScoreList"]
        miss = set(required_cols) - set(ruldatatype_df.columns)
        if miss:
            raise ValueError(f"ruldatatype_df í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {sorted(miss)}")

        rule_clean = ruldatatype_df["Rule"].fillna("").astype(str).str.strip()
        mask = (ruldatatype_df["MasterType"] != "Reference") & (rule_clean != "")
        df_rule = ruldatatype_df.loc[mask, required_cols].copy()
        if df_rule.empty:
            return pd.DataFrame(columns=out_cols)

        # Matched(%) ê³„ì‚° (MatchedScoreListì˜ ì²« ê°’)
        df_rule["Matched(%)"] = (
            df_rule["MatchedScoreList"].astype(str)
            .str.split(";").str[0].str.strip()
            .replace({"": np.nan, "nan": np.nan, "None": np.nan})
            .astype(float).fillna(0.0) * 100.0
        ).round(2)

        # rule name standardization
        rule_key_syn = {
            "ì£¼ì†Œ": "ADDRESS", "êµ­ê°€ì½”ë“œ": "COUNTRY_ISO3", "ì´ë©”ì¼": "EMAIL",
            "íœ´ëŒ€í°": "CELLPHONE", "ì „í™”": "TEL", "ìœ„ë„": "LATITUDE", "ê²½ë„": "LONGITUDE",
            "ì—°ì›”": "YEARMONTH", "ì—°ì›”ì¼": "DATECHAR", "ì„±ì”¨": "KOR_NAME",
            "ì„±ë³„êµ¬ë¶„": "GENDER", "ì„±ë³„êµ¬ë¶„_ì˜ë¬¸": "GENDER_EN",
        }
        def _std_rule_name(x: str) -> str:
            x = (x or "").strip()
            xu = x.upper()
            if xu in (t.upper() for t in valid_types):
                return xu
            return rule_key_syn.get(x, xu)

        df_rule["Rule"] = df_rule["Rule"].map(_std_rule_name)
        vtypes = {t.upper() for t in valid_types}
        rule_df = df_rule[df_rule["Rule"].isin(vtypes)].copy()
        if rule_df.empty:
            return pd.DataFrame(columns=out_cols)

        mapper = {
            'URL': validate_url, 'YEAR': validate_year, 'EMAIL': validate_email,
            'CELLPHONE': validate_cellphone, 'TEL': validate_tel,
            'LATITUDE': validate_latitude, 'LONGITUDE': validate_longitude,
            'DATECHAR': validate_date, 'YEARMONTH': validate_yearmonth,
            'YYMMDD': validate_YYMMDD, 'ADDRESS': validate_address,
            'KOR_NAME': validate_kor_name,
            'COUNTRY_ISO3': validate_country_code, 'êµ­ê°€ì½”ë“œ': validate_country_code,
            'ì‹œë„': validate_address, 'ì°¨ëŸ‰ë²ˆí˜¸': validate_car_number,
            'GENDER': validate_gender, 'GENDER_EN': validate_gender_en,
            'TIME': validate_time, 'TIMESTAMP': validate_timestamp,
        }

        results: List[Dict[str, Any]] = []
        # íŒŒì¼ ê²½ë¡œë³„ ì²˜ë¦¬ (group by FilePath)
        for fpath, grp in rule_df.sort_values('FilePath').groupby('FilePath'):
            src_path = str(fpath).strip()
            if not os.path.exists(src_path):
                self.logger.warning(f"[rule_mapping] íŒŒì¼ ê²½ë¡œ í™•ì¸ ë¶ˆê°€: {src_path}")
                continue

            # ì „ì²´ íŒŒì¼ ì½ê¸° (í—¤ë” ì •ë¦¬)
            try:
                df_src = pd.read_csv(src_path, encoding='utf-8-sig', on_bad_lines="skip", dtype=str, low_memory=False)
                df_src = _clean_headers(df_src)
            except Exception as e:
                self.logger.warning(f"[rule_mapping] íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {src_path} -> {e}")
                continue

            # sampling
            if sampling_rows and sampling_rows > 0 and len(df_src) > sampling_rows:
                df_src = df_src.sample(n=sampling_rows, random_state=42)

            for _, r in grp.iterrows():
                col = str(r['ColumnName']).replace('\ufeff','').strip()
                if col not in df_src.columns:
                    continue

                series = df_src[col].dropna().astype(str)
                non_null = len(series)
                if non_null == 0:
                    continue

                key = str(r['Rule']).strip().upper()
                fn = mapper.get(key)
                if fn is None:
                    self.logger.debug(f"[rule_mapping] ë¯¸ì§€ì› Rule: {key}")
                    continue

                try:
                    # apply validator and count True
                    valid_count = int(series.apply(fn).sum())
                except Exception as e:
                    self.logger.warning(f"[rule_mapping] validate ì‹¤íŒ¨: {src_path}::{col} ({key}) -> {e}")
                    continue

                if valid_count <= 0:
                    continue

                rate = round(valid_count / max(non_null, 1) * 100, 2)
                results.append({
                    'FilePath': src_path,
                    'FileName': os.path.basename(src_path),
                    'ColumnName': col,
                    'MasterType': str(r['MasterType']).strip(),
                    'MasterColumn': key,
                    'CompareCount': int(valid_count),
                    'MatchRate(%)': float(rate),
                    'MasterFilePath': 'Rule',
                    'MasterFile': 'Rule',
                    'ReferenceMasterType': 'Rule',
                    'CompareLength': '',
                    'SourceCount': int(non_null),
                })

        if not results:
            return pd.DataFrame(columns=out_cols)

        out = pd.DataFrame(results)[out_cols].copy()
        for c in ('CompareCount','SourceCount','MatchRate(%)'):
            out[c] = pd.to_numeric(out[c], errors='coerce')
        out['MatchRate(%)'] = out['MatchRate(%)'].astype("float32").round(2)
        return out

    # ------------------ (4) ìˆ«ì í†µê³„ ------------------
    def numeric_column_statistics(self, fileformat_df: pd.DataFrame, vSamplingRows: int = 10_000) -> Optional[pd.DataFrame]:
        """fileformat_dfì˜ DetailDataTypeì´ ë¹„ì–´ìˆëŠ” í•­ëª©ë“¤ì— ëŒ€í•´ ìˆ«ì í†µê³„ ê³„ì‚°"""
        def calc_numeric(file_path: str, cols: List[str]) -> Optional[pd.DataFrame]:
            try:
                if not os.path.exists(file_path):
                    self.logger.debug(f"[numeric] íŒŒì¼ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")
                    return None
                if file_path.lower().endswith('.csv'):
                    df = pd.read_csv(file_path, low_memory=False)
                elif file_path.lower().endswith('.pkl'):
                    df = pd.read_pickle(file_path)
                else:
                    df = pd.read_excel(file_path)
                if len(df) > vSamplingRows:
                    df = df.sample(n=vSamplingRows, random_state=42)

                rows=[]
                for c in cols:
                    if c not in df.columns:
                        continue
                    s = pd.to_numeric(df[c], errors='coerce').dropna()
                    if s.empty:
                        continue
                    desc = s.describe()
                    mean, std = desc['mean'], desc['std']
                    lcl, ucl = mean - 3*std, mean + 3*std
                    rows.append({
                        'FilePath': file_path, 'FileName': os.path.basename(file_path), 'ColumnName': c,
                        'dtype': str(s.dtype), 'Count': int(desc['count']), 'Mean': float(mean), 'Std': float(std),
                        'Min': float(desc['min']), '25%': float(desc['25%']), '50%': float(desc['50%']),
                        '75%': float(desc['75%']), 'Max': float(desc['max']),
                        'LCL': float(lcl), 'UCL': float(ucl),
                        'BelowLCL': int((s < lcl).sum()), 'AboveUCL': int((s > ucl).sum())
                    })
                return pd.DataFrame(rows) if rows else None
            except Exception as e:
                self.logger.warning(f"[numeric] ì²˜ë¦¬ ì˜¤ë¥˜: {file_path} -> {e}")
                return None

        self.logger.info("Numeric Column Statistics ì‹œì‘")
        # select candidates: DetailDataType empty AND Format(%) < 90 AND LenCnt > 2
        len_cnt = pd.to_numeric(fileformat_df.get('LenCnt', pd.Series(dtype='float')), errors='coerce').fillna(0)
        fmt_pct = pd.to_numeric(fileformat_df.get('Format(%)', pd.Series(dtype='float')), errors='coerce').fillna(0)
        target = fileformat_df[
            (len_cnt > 2) & (fmt_pct < 90) & (
                fileformat_df.get('DetailDataType').isna() |
                (fileformat_df.get('DetailDataType').astype(str).str.len() == 0)
            )
        ].copy()

        if target.empty:
            self.logger.info("Numeric: ì²˜ë¦¬ ëŒ€ìƒ ì—†ìŒ")
            return None

        blocks=[]
        for fpath, grp in target.groupby('FilePath'):
            cols = grp['ColumnName'].tolist()
            r = calc_numeric(fpath, cols)
            if r is not None and not r.empty:
                blocks.append(r)
        if not blocks:
            self.logger.info("Numeric ê²°ê³¼ ì—†ìŒ")
            return None
        return pd.concat(blocks, ignore_index=True)

    # ------------------ (5) Reference / Internal / Concat ------------------
    def reference_mapping(self, fileformat_df: pd.DataFrame) -> pd.DataFrame:
        self.logger.info("Reference Code Mapping ì‹œì‘")

        expand_df = Expand_Format(fileformat_df)
        expand_df['Format(%)'] = pd.to_numeric(expand_df.get('Format(%)', 0), errors='coerce').fillna(0)
        expand_df = expand_df.loc[expand_df['Format(%)'] > 10].copy()
        source_df = expand_df.loc[expand_df['MasterType'] != 'Reference'].copy()
        reference_df = expand_df.loc[expand_df['MasterType'] == 'Reference'].copy()
        combine_df = Combine_Format(source_df, reference_df)
        # Combine_Format must produce columns expected by mapping_check (MasterFilePath etc.)
        combine_df = combine_df[combine_df.get('Final_Flag', 0) == 1].copy()
        if combine_df.empty:
            return pd.DataFrame()
        mapping_df = self.mapping_check(combine_df)
        mapping_df = mapping_df[mapping_df['MatchRate(%)'] > MATCH_RATE_THRESHOLD]
        mapping_df = mapping_df.sort_values(by=['FilePath','ColumnName','MatchRate(%)'], ascending=[True,True,False])
        return mapping_df

    def internal_mapping(self, fileformat_df: pd.DataFrame) -> pd.DataFrame:
        self.logger.info("Internal Code Mapping ì‹œì‘")

        expand_df = Expand_Format(fileformat_df)
        expand_df['Format(%)'] = pd.to_numeric(expand_df.get('Format(%)', 0), errors='coerce').fillna(0)
        expand_df = expand_df.loc[expand_df['Format(%)'] > 10].copy()
        source_df = expand_df.loc[expand_df['MasterType'] != 'Reference'].copy()
        combine_df = Combine_Format(source_df, source_df) # Match ëœ ë ˆì½”ë“œ ì¤‘ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ë ˆì½”ë“œë§Œ ì„ íƒ
        combine_df = combine_df[combine_df.get('Final_Flag', 0) == 1].copy()
        if combine_df.empty:
            return pd.DataFrame()
        mapping_df = self.mapping_check(combine_df)
        mapping_df = mapping_df[mapping_df['MatchRate(%)'] > MATCH_RATE_THRESHOLD]
        mapping_df = mapping_df.sort_values(by=['FilePath','ColumnName','MatchRate(%)'], ascending=[True,True,False])
        return mapping_df

    def mapping_concat(self, reference_df: pd.DataFrame, internal_df: pd.DataFrame, rule_df: pd.DataFrame) -> pd.DataFrame:
        self.logger.info("ëª¨ë“  ë§¤í•‘ íŒŒì¼ì„ í†µí•©í•©ë‹ˆë‹¤.")
        required_cols = [
            'FilePath','FileName','ColumnName','MasterType','MasterFilePath',
            'MasterFile','ReferenceMasterType','MasterColumn','CompareLength',
            'CompareCount','MatchRate(%)'
        ]
        # safe slicing: if any missing -> create empty frame with those columns
        def safe_slice(df):
            if df is None or df.empty:
                return pd.DataFrame(columns=required_cols)
            cols_present = [c for c in required_cols if c in df.columns]
            missing = [c for c in required_cols if c not in df.columns]
            out = df.copy()
            for m in missing:
                out[m] = "" if "Count" not in m and "Rate" not in m else 0
            return out[required_cols]

        rref = safe_slice(reference_df)
        rint = safe_slice(internal_df)
        rrul = safe_slice(rule_df)
        concat_df = pd.concat([rref, rint, rrul], ignore_index=True)
        # rename to pivot-friendly names
        concat_df = concat_df.rename(columns={
            'MasterFilePath':'CodeFilePath','MasterFile':'CodeFile',
            'ReferenceMasterType':'CodeType','MasterColumn':'CodeColumn',
            'CompareCount':'Matched','MatchRate(%)':'Matched(%)'
        })
        # keep meaningful candidates only (>=20% match)
        concat_df['Matched(%)'] = pd.to_numeric(concat_df.get('Matched(%)', 0), errors='coerce').fillna(0)
        concat_df = concat_df[concat_df['Matched(%)'] > MATCH_RATE_THRESHOLD]
        concat_df = concat_df.sort_values(by=['FilePath','FileName','ColumnName','MasterType','Matched(%)'],
                                          ascending=[True,True,True,True,False])
        return concat_df

    # 2025-11-26 ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•¨ 
    def final_mapping(self, fileformat_df, pivoted_df, reference_df, rule_df) -> pd.DataFrame:
        """fileformat_dfì™€ ruldatatype(preset)ê³¼ pivoted_dfë¥¼ í•©ì³ ìµœì¢… ì‚°ì¶œ"""
        #---------------------------------------------------------
        #  rule_df ì½ì–´ì˜´. 
         #---------------------------------------------------------
        self.logger.info("ìµœì¢… ë§¤í•‘ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        df_rule = rule_df.copy()
        if df_rule.empty:
            self.logger.debug("ruldatatype ë¹„ì–´ìˆìŒ -> ë£° ë°˜ì˜ ìŠ¤í‚µ")
        rule_required_cols = ["FilePath","FileName","ColumnName","MasterType", "Rule","MatchedScoreList"]
        # safe: fill missing rule cols if necessary
        for c in rule_required_cols:
            if c not in df_rule.columns:
                df_rule[c] = ""

        df_rule = df_rule[rule_required_cols].copy()
        #---------------------------------------------------------
        #  reference_df ì½ì–´ì˜´. 
        #---------------------------------------------------------
        ref_cols = ["FilePath","FileName","ColumnName","MasterType","MasterFilePath","MasterFile","ReferenceMasterType","MasterColumn","CompareLength","CompareCount","SourceCount","MatchRate(%)"]
        ref_df = reference_df[ref_cols].copy()
        ref_df = ref_df.sort_values(by=['FilePath','FileName','ColumnName','MasterType','MatchRate(%)'], ascending=[True,True,True,True,False])
        ref_df = ref_df.groupby(['FilePath', 'ColumnName'], as_index=False).head(1)
        if ref_df.empty:
            self.logger.debug("reference_df ë¹„ì–´ìˆìŒ -> ì°¸ì¡° ë°˜ì˜ ìŠ¤í‚µ")
        ref_df = ref_df.rename(columns={
            'MasterFilePath':'CodeFilePath_4',
            'MasterFile':'CodeFile_4',
            'ReferenceMasterType':'CodeType_4',
            'MasterColumn':'CodeColumn_4',
            'CompareCount':'Matched_4',
            'MatchRate(%)':'Matched(%)_4'
        })

        #---------------------------------------------------------
        #  pivoted_df ì½ì–´ì˜´. 
        #---------------------------------------------------------
        # pivoted_df may be empty -> create empty with expected columns
        pivot_cols = [
            'FilePath','FileName','ColumnName','MasterType',
            'CodeColumn_1','CodeFile_1','CodeFilePath_1','CodeType_1','Matched_1','Matched(%)_1',
            'CodeColumn_2','CodeFile_2','CodeFilePath_2','CodeType_2','Matched_2','Matched(%)_2'
        ]
        if pivoted_df is None or pivoted_df.empty:
            pivoted_df = pd.DataFrame(columns=pivot_cols)
        else:  # ensure all pivot cols exist
            for c in pivot_cols:
                if c not in pivoted_df.columns:
                    pivoted_df[c] = ""

        # merge
        df = pd.merge(fileformat_df, df_rule, on=['FilePath','FileName','ColumnName','MasterType'], how='left', suffixes=("","_rule"))
        df = pd.merge(df, pivoted_df, on=['FilePath','FileName','ColumnName','MasterType'], how='left', suffixes=("","_pivot"))
        df = pd.merge(df, ref_df, on=['FilePath','FileName','ColumnName','MasterType'], how='left', suffixes=("","_ref"))

        #---------------------------------------------------------
        #  Attribute ì»¬ëŸ¼ ìƒì„±
        #---------------------------------------------------------
        # Rule ì»¬ëŸ¼ì—ì„œ ì„¸ë¯¸ì½œë¡  ê¸°ì¤€ ì²« ë²ˆì§¸ ê°’ ì¶”ì¶œí•˜ì—¬ Attributeì— ì„¤ì •
        df['Attribute'] = ""
        if 'Rule' in df.columns:
            # Rule ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  NaN ì²˜ë¦¬
            df['Rule'] = df['Rule'].fillna("").astype(str).str.strip()
            # ì„¸ë¯¸ì½œë¡  ê¸°ì¤€ìœ¼ë¡œ ì²« ë²ˆì§¸ ê°’ ì¶”ì¶œ
            rule_first_value = df['Rule'].str.split(';').str[0].str.strip()
            # ê°’ì´ ìˆìœ¼ë©´ Attributeì— ì„¤ì •
            mask_rule = rule_first_value != ""
            df.loc[mask_rule, 'Attribute'] = rule_first_value[mask_rule]
        
        # Ruleì—ì„œ ê°’ì´ ì—†ëŠ” ê²½ìš° CodeColumn_4 ê°’ ì‚¬ìš©
        if 'CodeColumn_4' in df.columns:
            df['CodeColumn_4'] = df['CodeColumn_4'].fillna("").astype(str).str.strip()
            mask_no_rule = (df['Attribute'] == "") & (df['CodeColumn_4'] != "")
            df.loc[mask_no_rule, 'Attribute'] = df.loc[mask_no_rule, 'CodeColumn_4']

        #---------------------------------------------------------
        #  PK -> FK mapping (if PK column present in fileformat_df) ì¶”ê°€
        #---------------------------------------------------------
        if 'PK' in fileformat_df.columns:
            pk_numeric = pd.to_numeric(fileformat_df['PK'], errors='coerce').fillna(0).astype(int)
            mask_pk = pk_numeric == 1
            tmp_df = fileformat_df.loc[mask_pk, ['FilePath','ColumnName']].copy()
            tmp_df = tmp_df.rename(columns={'FilePath':'CodeFilePath_1','ColumnName':'CodeColumn_1'})
            tmp_df['FK'] = 'FK'
            df = pd.merge(df, tmp_df, on=['CodeFilePath_1','CodeColumn_1'], how='left')

        return df

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    import time
    start_time = time.time()
    main_config = Load_Yaml_File(DQConfig.get_path(DQConfig.YAML_RELATIVE_PATH))
    analyzer = Initializing_Main_Class(main_config)
    analyzer.process_files_mapping()

    print("="*50)
    print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {time.time()-start_time:.2f}ì´ˆ")
    print("="*50)


# ë‹¤ìŒì€ ì„±ëŠ¥í–¥ìƒì„ ìœ„í•˜ì—¬ ì ìš©í•œ ê¸°ë²•ë“¤ ì…ë‹ˆë‹¤.  
# ğŸ“Š ì„±ëŠ¥ ìµœì í™” ìš”ì•½ (43s â†’ 17.9s)ìµœì í™” ë‹¨ê³„ì ìš© ê¸°ìˆ íš¨ê³¼
# 1ë‹¨ê³„: í•„í„°ë§set ê¸°ë°˜ ê²€ìƒ‰ ë° ì¤‘ë³µ copy() 
# ì œê±°ì´ˆê¸° ë°ì´í„° ë¡œë”© ë° ë©”ëª¨ë¦¬ ì ìœ ìœ¨ ê°ì†Œ
# 2ë‹¨ê³„: ë³‘í•©(Merge)í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ì¡°ì¸
# ì¡°ì¸ ì—°ì‚° ì‹œ ë°œìƒí•˜ëŠ” ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
# 3ë‹¨ê³„: ì—°ì‚°(Flag)between, & ë…¼ë¦¬ ì—°ì‚°ì í™œìš©
# CPU ìˆ˜ì¤€ì˜ ë¹„íŠ¸ ì—°ì‚°ìœ¼ë¡œ ê³„ì‚° ì†ë„ ê·¹ëŒ€í™”
# 4ë‹¨ê³„: ë¬¸ìì—´ ì²˜ë¦¬
# np.whereì™€ ë²¡í„°í™”ëœ .str ì ‘ê·¼ë¬¸ìì—´ ë£¨í”„ ì²˜ë¦¬ ë¹„ìš© ì ˆê°