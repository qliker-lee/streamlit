# -*- coding: utf-8 -*-
"""
DataSense DQ Profiling System - Refactored for Maintenance & EXE
- ëª¨ë“  ê¸°ì¡´ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë³´ì¡´
- í´ë˜ìŠ¤ ê¸°ë°˜ ëª¨ë“ˆí™”ë¡œ ê°€ë…ì„± í–¥ìƒ
- ì‹¤í–‰ íŒŒì¼(EXE) ê²½ë¡œ ëŒ€ì‘ ë¡œì§ í¬í•¨
"""

import os
import re
import sys
import yaml
import time
import json  # <--- ì´ ì¤„ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
import traceback
import platform
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
from multiprocessing import Pool, cpu_count
from collections import Counter

# --- [1. ê²½ë¡œ ë° ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤] ---
class DQConfig:
    ROOT_PATH = Path(__file__).resolve().parents[2]
    YAML_RELATIVE_PATH = 'DataSense/util/DS_Master.yaml'
    CONTRACT_RELATIVE_PATH = 'DataSense/util/DQ_Contract.yaml'

    @staticmethod
    def get_path(rel_path):
        """EXE ë¹Œë“œ í™˜ê²½ê³¼ ì¼ë°˜ íŒŒì´ì¬ í™˜ê²½ ëª¨ë‘ ëŒ€ì‘"""
        if hasattr(sys, '_MEIPASS'):
            return os.path.join(sys._MEIPASS, rel_path)
        return os.path.join(DQConfig.ROOT_PATH, rel_path)

# sys.path ì¶”ê°€ (ë‚´ë¶€ ëª¨ë“ˆ ì°¸ì¡°ìš©)
if str(DQConfig.ROOT_PATH) not in sys.path:
    sys.path.insert(0, str(DQConfig.ROOT_PATH))

# ì™¸ë¶€ ìœ í‹¸ ì„í¬íŠ¸ (ê¸°ì¡´ í•¨ìˆ˜ë“¤)
try:
    from DataSense.util.io import Load_Yaml_File, Backup_File
    from DataSense.util.dq_function import (
        DataType_Analysis, create_standard_df, Determine_Detail_Type,
        Get_Oracle_Type, _ensure_severity_column, build_top_issue_reports, 
        save_or_load_baseline, compute_proxy_drift, make_df_for_distribution, 
        build_dist_snapshot_for_df, collect_value_samples, dist_topk_categories, 
        add_dq_scores, apply_score_importance, compute_snapshot_drift
    )
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    sys.exit(1)

# --- [2. ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤: ë°˜ë³µ ë¡œì§ ì²˜ë¦¬] ---
class DQUtils:
    _FLOAT_ZERO_RE = re.compile(r'^[+-]?\d+\.0+$')

    @staticmethod
    def strip_decimal_zero(x):
        s = str(x)
        return s.split('.', 1)[0] if DQUtils._FLOAT_ZERO_RE.fullmatch(s) else s

    @staticmethod
    def get_pattern(value):
        """ë¬¸ìì—´ íŒ¨í„´ ì¶”ì¶œ (n:ìˆ«ì, K:í•œê¸€, A/a:ì˜ë¬¸ ë“±)"""
        s = DQUtils.strip_decimal_zero(value)[:20]
        p = []
        for ch in s:
            if ch.isdigit(): p.append('n')
            elif 'ê°€' <= ch <= 'í£': p.append('K')
            elif ch.isalpha(): p.append('A' if ch.isupper() else 'a')
            elif ch in '(){}[]-=. :@/': p.append(ch)
            else: p.append('s')
        res = "".join(p)
        return f"'{res}" if res.startswith('-') else res

# --- [3. ë¶„ì„ ì—”ì§„ í´ë˜ìŠ¤: ì»¬ëŸ¼ë³„ í†µê³„ ê³„ì‚°] ---
class ColumnProfiler:
    def __init__(self, df, column):
        self.col = column
        self.series = df[column]
        self.non_null = self.series.dropna()
        self.str_vals = self.non_null.astype(str)
        self.count = len(df)

    def profile(self):
        if self.count == 0: return {}

        val_cnt = len(self.non_null)
        null_cnt = self.count - val_cnt
        unique_cnt = self.non_null.nunique()
        
        res = {
            'ColumnName': self.col,
            'DataType': str(self.series.dtype),
            'OracleType': Get_Oracle_Type(self.series, self.col),
            'PK': 1 if unique_cnt == self.count and self.count > 0 else 0,
            'RecordCnt': self.count,
            'ValueCnt': val_cnt,
            'Null(%)': round(null_cnt / self.count * 100, 2) if self.count > 0 else 0,
            'UniqueCnt': unique_cnt,
            'Unique(%)': round(unique_cnt / val_cnt * 100, 2) if val_cnt > 0 else 0,
        }

        if val_cnt > 0:
            lens = self.str_vals.str.len()
            top_counts = self.str_vals.value_counts().head(10)
            top10_list = top_counts.index.tolist()
            top10_json = json.dumps(top10_list, ensure_ascii=False)
            
            # íŒ¨í„´ ë¶„ì„
            patterns = self.str_vals.map(DQUtils.get_pattern)
            pattern_stats_list = Counter(patterns).most_common()
            top_p = pattern_stats_list[0][0] if pattern_stats_list else ""
            format_cnt = len(pattern_stats_list)

            # --- [ì¶”ê°€ ì»¬ëŸ¼ ë¡œì§ ì‹œì‘] ---
            # 1. CompareLength: ê¸¸ì´ì˜ ê³ ì • ì—¬ë¶€ (Minê³¼ Maxê°€ ê°™ìœ¼ë©´ 0, ë‹¤ë¥´ë©´ 1)
            res['CompareLength'] = 1 if lens.min() != lens.max() else 0

            # --- [í•µì‹¬] 1, 2, 3ìˆœìœ„ í¬ë§·ë³„ ìƒì„¸ í†µê³„ ìƒì„± ë£¨í”„ ---
            for i in range(1, 4):
                suffix = "" if i == 1 else "_2" if i == 2 else "_3"
                # ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ ê·œê²©ì— ë§ì¶˜ ì»¬ëŸ¼ ì ‘ë¯¸ì‚¬ (_1, _2, _3)
                col_suffix = f"_{i}" 
                
                if len(pattern_stats_list) >= i:
                    p_val, p_cnt = pattern_stats_list[i-1]
                    
                    # í•´ë‹¹ í¬ë§· ëª…ì¹­ ë° ê¸°ë³¸ í†µê³„
                    res[f'Format{suffix}'] = p_val
                    res[f'Format{suffix}Value'] = p_cnt
                    res[f'Format{suffix}(%)'] = round(p_cnt / val_cnt * 100, 2)

                    # í•´ë‹¹ í¬ë§·ì— í•´ë‹¹í•˜ëŠ” ì‹¤ì œ ê°’ë“¤ í•„í„°ë§
                    fmt_vals = self.str_vals[patterns == p_val]
                    if not fmt_vals.empty:
                        res[f'FormatMin{col_suffix}'] = fmt_vals.min()[:50]
                        res[f'FormatMax{col_suffix}'] = fmt_vals.max()[:50]
                        # ì¤‘ê°„ê°’ ì¶”ì¶œ
                        sorted_vals = fmt_vals.sort_values()
                        res[f'FormatMedian{col_suffix}'] = sorted_vals.iloc[len(sorted_vals) // 2]
                    else:
                        res[f'FormatMin{col_suffix}'] = ""
                        res[f'FormatMax{col_suffix}'] = ""
                        res[f'FormatMedian{col_suffix}'] = ""
                else:
                    # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆê°’ ì²˜ë¦¬
                    res[f'Format{suffix}'] = ""
                    res[f'Format{suffix}Value'] = 0
                    res[f'Format{suffix}(%)'] = 0.0
                    res[f'FormatMin{col_suffix}'] = ""
                    res[f'FormatMax{col_suffix}'] = ""
                    res[f'FormatMedian{col_suffix}'] = ""

            # --- [í•µì‹¬] Determine_Detail_Typeìš© ì¸ì ìƒì„± ---
            # 1. format_stats: í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤ë“¤ ë§¤í•‘
            f_stats = {
                'FormatMedian': self.str_vals.sort_values().iloc[val_cnt // 2], # ì¤‘ê°„ê°’ ë¬¸ìì—´
                'FormatMode': self.str_vals.mode()[0] if not self.str_vals.mode().empty else "",
                'most_common_pattern': top_p,
                'pattern_type_cnt': format_cnt
            }
            # 2. total_stats: í”Œë˜ê·¸ ë° ì‹œí€€ìŠ¤ íŒë³„ìš©
            t_stats = {
                'min': self.str_vals.min(),
                'max': self.str_vals.max(),
                'mode': f_stats['FormatMode']
            }

            try:
                # í•¨ìˆ˜ í˜¸ì¶œ (8ê°œ ì¸ì ìˆœì„œ ì¤€ìˆ˜)
                dt_result = Determine_Detail_Type(
                    top_p,            # 1. pattern
                    format_cnt,       # 2. pattern_type_cnt
                    f_stats,          # 3. format_stats
                    t_stats,          # 4. total_stats
                    int(lens.max()),  # 5. max_length
                    int(unique_cnt),  # 6. unique_count
                    int(val_cnt),     # 7. non_null_count
                    top10_json        # 8. top10 (is_tel í•¨ìˆ˜ê°€ json.loadsë¥¼ ìˆ˜í–‰í•¨)
                )
                res['DetailDataType'] = dt_result if dt_result else ""
            except Exception as e:
                res['DetailDataType'] = "Error"
                print(f"âš ï¸ {self.col} íƒ€ì… íŒë³„ ì¤‘ ì˜¤ë¥˜: {e}")

            # --- í¬ë§· í†µê³„ (1st, 2nd, 3rd) ---
            res['FormatCnt'] = format_cnt
            for i in range(1, 4):
                suffix = "" if i == 1 else "2nd" if i == 2 else "3rd"
                if len(pattern_stats_list) >= i:
                    p_val, p_cnt = pattern_stats_list[i-1]
                    res[f'Format{suffix}'] = p_val
                    res[f'Format{suffix}Value'] = p_cnt
                    res[f'Format{suffix}(%)'] = round(p_cnt / val_cnt * 100, 2)
                else:
                    res[f'Format{suffix}'] = ""; res[f'Format{suffix}Value'] = 0; res[f'Format{suffix}(%)'] = 0

            # ë‚˜ë¨¸ì§€ í†µê³„ (Min/Max/Top10/Slicing)
            res.update({
                'LenMin': int(lens.min()), 'LenMax': int(lens.max()), 'LenAvg': round(lens.mean(), 1),
                'MinString': self.str_vals.min()[:50], 'MaxString': self.str_vals.max()[:50],
                'ModeString': f_stats['FormatMode'], 'ModeCnt': int(top_counts.iloc[0]) if not top_counts.empty else 0,
                'Top10': top10_json, 'Top10(%)': round(top_counts.sum() / val_cnt * 100, 2)
            })
            
            for n in [1, 2, 3]:
                res.update(self._get_edge_stats('First', n))
                res.update(self._get_edge_stats('Last', n))
        
        return res

    def _get_edge_stats(self, side, n):
        stats = {}
        func = (lambda x: x[:n]) if side == 'First' else (lambda x: x[-n:])
        top = self.str_vals.apply(func).value_counts().head(3)
        for i in range(3):
            val = str(top.index[i]) if i < len(top) else ""
            cnt = int(top.iloc[i]) if i < len(top) else 0
            stats[f'{side}{n}M{i+1}'] = val
            stats[f'{side}{n}Cnt{i+1}'] = cnt
        return stats
        
# --- [4. ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì—”ì§„] ---
class DQEngine:
    def __init__(self, config_dict):
        self.config = config_dict
        # output_pathë¥¼ ì•ˆì „í•˜ê²Œ ì„¤ì •
        root = config_dict.get('ROOT_PATH', '')
        out_dir = config_dict.get('directories', {}).get('output', 'DS_Output')
        self.output_path = Path(root) / out_dir
        self.sampling_rows = 10000
        self.file_stats_list = [] 

    def process_file(self, file_meta):
        """
        ìˆ˜ì • ì‚¬í•­: ë¶„ì„ ê²°ê³¼(column_results)ì™€ íŒŒì¼ í†µê³„(file_stats)ë¥¼ íŠœí”Œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        source_dir = file_meta.get('source') or file_meta.get('file_path') or file_meta.get('FilePath')
        code_type = file_meta.get('type') or file_meta.get('code_type') or file_meta.get('Master_Type')
        extension = file_meta.get('ext', '.csv').lower()
        if extension and not extension.startswith('.'):
            extension = '.' + extension

        if not source_dir:
            return [], [] # ë¹ˆ ê²°ê³¼ ë°˜í™˜

        target_files = []
        try:
            if os.path.isdir(source_dir):
                for f in os.listdir(source_dir):
                    if f.lower().endswith(extension):
                        target_files.append(os.path.join(source_dir, f))
            elif os.path.isfile(source_dir):
                target_files.append(source_dir)
        except Exception:
            return [], []

        all_column_results = []
        local_file_stats = [] # ì´ í”„ë¡œì„¸ìŠ¤ ë‚´ì—ì„œ ìˆ˜ì§‘í•  í†µê³„

        for f_path in target_files:
            try:
                file_size = os.path.getsize(f_path)
                if f_path.lower().endswith('.csv'):
                    df = pd.read_csv(f_path, dtype=str, low_memory=False, encoding='utf-8-sig')
                else:
                    df = pd.read_excel(f_path, dtype=str)

                record_cnt = len(df)
                column_cnt = len(df.columns)
                sampling_rows = min(record_cnt, self.sampling_rows)
                sample_df = df if record_cnt <= self.sampling_rows else df.sample(n=self.sampling_rows)
                
                # íŒŒì¼ í†µê³„ ìˆ˜ì§‘
                local_file_stats.append({
                    'FilePath': f_path,
                    'FileName': os.path.basename(f_path),
                    'MasterType': code_type,
                    'FileSize': file_size,
                    'RecordCnt': record_cnt,
                    'ColumnCnt': column_cnt,
                    'SamplingRows': sampling_rows,
                    'Sampling(%)': round((sampling_rows / record_cnt * 100), 2) if record_cnt > 0 else 0,
                    'WorkDate': datetime.now().strftime('%Y-%m-%d')
                })

                for col in sample_df.columns:
                    profiler = ColumnProfiler(sample_df, col)
                    col_res = profiler.profile()
                    col_res.update({
                        'MasterType': code_type,
                        'FileName': os.path.basename(f_path),
                        'FilePath': f_path,
                        'TotalRecords': record_cnt
                    })
                    all_column_results.append(col_res)
                
            except Exception as e:
                print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ ({os.path.basename(f_path)}): {e}")
                continue
        
        # ì¤‘ìš”: ì»¬ëŸ¼ ë¶„ì„ ê²°ê³¼ì™€ íŒŒì¼ í†µê³„ë¥¼ ëª¨ë‘ ë¦¬í„´í•¨
        return all_column_results, local_file_stats

    def run(self, codelist):
        print(f"ğŸš€ ë¶„ì„ ì‹œì‘ (CPU Core: {cpu_count()})")
        
        with Pool(cpu_count()) as pool:
            # resultsëŠ” [(col_res1, stat_res1), (col_res2, stat_res2), ...] í˜•íƒœê°€ ë¨
            combined_results = pool.map(self.process_file, codelist)
        
        # 1. ë¶„ì„ ê²°ê³¼ í†µí•© (Flatten)
        flat_column_results = []
        all_file_stats = []
        
        for col_res, stat_res in combined_results:
            flat_column_results.extend(col_res)
            all_file_stats.extend(stat_res)
        
        if not flat_column_results:
            print("âŒ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2. DQ ê²°ê³¼ ì €ì¥
        if flat_column_results:
            final_df = pd.DataFrame(flat_column_results)
            final_df = add_dq_scores(final_df)
            final_df.insert(0, 'No', range(1, len(final_df) + 1))
            # ìµœì¢… final_df ì˜ ì»¬ëŸ¼ìˆœì„œê°€ No, FilePath, FileName, MasterType, ColumnName, DataType, OracleType, DetailDataType, PK, ValueCnt ì´í›„ëŠ” ìˆœì„œ
            column_order = ['No', 'FilePath', 'FileName', 'MasterType', 'ColumnName', 'DataType', 'OracleType', 'DetailDataType', 'PK', 'ValueCnt']
            
            # ì§€ì •ëœ ì»¬ëŸ¼ë“¤ì„ ì•ì— ë°°ì¹˜í•˜ê³ , ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤ì€ ì›ë˜ ìˆœì„œëŒ€ë¡œ ìœ ì§€
            existing_columns = final_df.columns.tolist()
            ordered_columns = []
            
            # 1. column_orderì— ìˆëŠ” ì»¬ëŸ¼ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì¶”ê°€ (ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ)
            for col in column_order:
                if col in existing_columns:
                    ordered_columns.append(col)
            
            # 2. ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤ì„ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì¶”ê°€
            for col in existing_columns:
                if col not in ordered_columns:
                    ordered_columns.append(col)
            
            # ì»¬ëŸ¼ ìˆœì„œ ì ìš©
            final_df = final_df[ordered_columns]
            
            save_path = self.output_path / "FileFormat.csv"
            final_df.to_csv(save_path, index=False, encoding='utf-8-sig')
            print(f"âœ… DQ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}")

        # 3. FileStats ì €ì¥
        if all_file_stats:
            stats_df = pd.DataFrame(all_file_stats)
            stats_df.insert(0, 'FileNo', range(1, len(stats_df) + 1))
            
            # ê²½ë¡œ ì„¤ì • ë³´ê°•
            out_path = self.config.get('output_path') or str(self.output_path)
            stats_final_path = os.path.join(out_path, "FileStats.csv")
            
            stats_df.to_csv(stats_final_path, index=False, encoding="utf-8-sig")
            print(f"âœ… íŒŒì¼ ë‹¨ìœ„ í†µê³„ ì €ì¥ ì™„ë£Œ: {stats_final_path}")


# --- [5. ì‹¤í–‰ë¶€] ---
if __name__ == "__main__":
    start_time = time.time()
    
    # ì„¤ì • ë¡œë“œ
    main_config = Load_Yaml_File(DQConfig.get_path(DQConfig.YAML_RELATIVE_PATH))
    
    # ì‹¤í–‰ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ í™•ë³´ (Excelì—ì„œ Yì¸ ê²ƒë§Œ)
    meta_path = os.path.join(main_config['ROOT_PATH'], main_config['files']['codelist_meta'])
    codelist_df = pd.read_excel(meta_path)
    codelist_list = codelist_df[codelist_df['execution_flag'] == 'Y'].to_dict(orient='records')

    # ì—”ì§„ êµ¬ë™
    engine = DQEngine(main_config)
    engine.run(codelist_list)

    print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")

    